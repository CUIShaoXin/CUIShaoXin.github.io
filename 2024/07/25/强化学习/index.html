<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="强化学习强化学习的终极目标：求解最优策略 ——第一节课（p1）1.agent​	基本上就是用于决策的模型整体 2.state​	 当前的状态, 以平面迷宫为例, 就是当前所处的格子。 ​		1. Stata space: 所有状态的集合。（ i 的范围从 1—9）  3.action​	可采取的行动, 还是以平面迷宫为例, 有上下左右移动或者原地不动5个action。 4.state transi">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记">
<meta property="og:url" content="http://example.com/2024/07/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="CuiShao Xin&#39;s Blog">
<meta property="og:description" content="强化学习强化学习的终极目标：求解最优策略 ——第一节课（p1）1.agent​	基本上就是用于决策的模型整体 2.state​	 当前的状态, 以平面迷宫为例, 就是当前所处的格子。 ​		1. Stata space: 所有状态的集合。（ i 的范围从 1—9）  3.action​	可采取的行动, 还是以平面迷宫为例, 有上下左右移动或者原地不动5个action。 4.state transi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617145748004.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617153118346.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154110728.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154611741.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202537238.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202552718.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203241653.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203319922.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211131500.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211148758.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619153739063.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162401938.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162438168.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162741338.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162846264.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163011002.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163222662.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163944486.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619164158443.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619170916628.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191913289.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191956982.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619193816799.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195156253.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195751413.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622202846552.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203018759.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203248744.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203509509.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622204420356.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622211359926.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214605992.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214645269.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715104533466.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715105918534.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110206450.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110912989.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715111357867.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155354938.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155420097.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170156816.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161525151.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161737910.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170605152.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171114197.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171321412.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240716160217245.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095247150.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095738268.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150326386.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150448821.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150840806.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717153539867.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717203326321.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204558842.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204857582.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717205846572.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104532551.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104614360.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105138076.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105829512.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719153155744.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719154439357.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719160609559.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719161205019.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162237012.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162344758.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162848871.png">
<meta property="article:published_time" content="2024-07-25T03:29:33.993Z">
<meta property="article:modified_time" content="2024-07-25T07:54:20.141Z">
<meta property="article:author" content="ShaoXin Cui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617145748004.png">

<link rel="canonical" href="http://example.com/2024/07/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习笔记 | CuiShao Xin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">CuiShao Xin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/07/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ShaoXin Cui">
      <meta itemprop="description" content="有志者事竟成">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CuiShao Xin's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-07-25 11:29:33 / 修改时间：15:54:20" itemprop="dateCreated datePublished" datetime="2024-07-25T11:29:33+08:00">2024-07-25</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p><u>强化学习的终极目标：求解最优策略</u></p>
<h3 id="——第一节课（p1）"><a href="#——第一节课（p1）" class="headerlink" title="——第一节课（p1）"></a>——第一节课（p1）</h3><h4 id="1-agent"><a href="#1-agent" class="headerlink" title="1.agent"></a>1.agent</h4><p>​	基本上就是用于决策的模型整体</p>
<h4 id="2-state"><a href="#2-state" class="headerlink" title="2.state"></a><em>2</em>.state</h4><p>​	 当前的状态, 以平面迷宫为例, 就是当前所处的格子。</p>
<p>​		<em>1.</em> Stata space: 所有状态的集合。（ i 的范围从 1—9）</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617145748004.png" alt="image-20240617145748004"></p>
<h4 id="3-action"><a href="#3-action" class="headerlink" title="3.action"></a>3.action</h4><p>​	可采取的行动, 还是以平面迷宫为例, 有上下左右移动或者原地不动5个action。</p>
<h4 id="4-state-transition"><a href="#4-state-transition" class="headerlink" title="4.state transition"></a>4.state transition</h4><p>​	从某一个state到另外一个state。</p>
<h4 id="5-forbidden-area"><a href="#5-forbidden-area" class="headerlink" title="5.forbidden area"></a>5.forbidden area</h4><ul>
<li>情况一：可以进入但是会受到惩罚。（我们考虑的是这种情况）</li>
<li>情况二：不可以进入。</li>
</ul>
<h4 id="6-state-transition-probability"><a href="#6-state-transition-probability" class="headerlink" title="6.state transition probability"></a>6.state transition probability</h4><ul>
<li><p>白话文：我们此时处于S1状态，如果我们选择向右走，则下一个状态则是S2。</p>
<p>​	在数学中，在状态为S1和下一步向右走，得到S2的状态概率为1。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617153118346.png" alt="image-20240617153118346"></p>
</li>
</ul>
<h4 id="7-policy"><a href="#7-policy" class="headerlink" title="7.policy"></a>7.policy</h4><p>​	告诉agent处于某一个状态下，应该做出怎样的action。</p>
<ul>
<li><p>Π相当于是一个策略，他会告诉我们处于这个state下，各个action的<em>概率</em>。<strong>（这是个确定的情况）</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154110728.png" alt="image-20240617154110728"></p>
</li>
<li><p><strong>（不确定的情况）</strong></p>
</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154611741.png" alt="image-20240617154611741"></p>
<h3 id="——第一节课（p2）"><a href="#——第一节课（p2）" class="headerlink" title="——第一节课（p2）"></a>——第一节课（p2）</h3><h4 id="1-reward"><a href="#1-reward" class="headerlink" title="1.reward"></a>1.reward</h4><ul>
<li><p>正数：<strong>在采取某一个动作之后</strong>，会获得奖励。</p>
</li>
<li><p>负数：<strong>在采取某一个动作之后</strong>，会获得惩罚。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202537238.png" alt="image-20240617202537238"></p>
</li>
</ul>
<p>​		①在状态S1的情况下，如果我们选择向上走，则奖励会是-1。</p>
<p>​		②在概率模型下，如果状态S1的情况下，我们选择向上走，则得到奖励是-1的概率是1。 </p>
<p>​			<strong>（reward一定是依赖于当前的state和action，而不是依赖与下一状态。）</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202552718.png" alt="image-20240617202552718"></p>
<h4 id="2-trajectory"><a href="#2-trajectory" class="headerlink" title="2.trajectory"></a>2.trajectory</h4><p>​	一系列状态(state)和行为(action)交替的轨迹, 类似一条线路。                                                                  <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203241653.png" alt="image-20240617203241653">	</p>
<p>​			红色的线即为<strong>trajectory</strong>。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203319922.png" alt="image-20240617203319922"></p>
<h4 id="3-return"><a href="#3-return" class="headerlink" title="3.return"></a>3.return</h4><p>​	<u><strong>return是对于trajectory而言的。</strong></u></p>
<p>​	对于上图的trajectory而言，return&#x3D;0+0+0+1&#x3D;1。</p>
<p>​	<strong>return的作用：我们可以通过return来判断一个policy与另外一个policy哪一个更好</strong>。</p>
<h4 id="4-discounted-return"><a href="#4-discounted-return" class="headerlink" title="4.discounted return"></a>4.discounted return</h4><p>​	说到底就是discount rate和return之间的结合。首先这个discount rate ：η是一个大于0小于1 的数。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211131500.png" alt="image-20240617211131500"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211148758.png" alt="image-20240617211148758"></p>
<p>其中上图中的0，1<strong>都是reward</strong>。</p>
<p><u>引入discounted return的作用</u></p>
<ul>
<li>在trajectory<strong>为无限的情况下</strong>，所得的return为一个不收敛的值。加上discount rate之后现在是一个有限的值。</li>
<li>如果η趋近于0，则agent会越来越注重  <strong>现在</strong>  的reward。</li>
<li>如果η趋近于1，则agent会越来越注重  <strong>未来</strong>  的reward。</li>
</ul>
<h4 id="5-episode"><a href="#5-episode" class="headerlink" title="5.episode"></a>5.episode</h4><p>​	episode是带有<strong>terminal states（最终状态）</strong>的trajectory。</p>
<p>​			episodicTasks : 带有终止状态的任务。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>​	在MDP（马尔科夫决策过程）框架之下：	</p>
<p>​		<strong>1.要素：</strong></p>
<p>​			 <strong>状态 state space</strong> - 可以直观的理解为地图</p>
<p>​    		 地图本身就是一个空间(space)</p>
<p>​			<strong>动作 action space</strong> - 上下左右以及原地不动 (不是固定值, 而是概率分布)</p>
<p>​			不同状态(state) , 能够采取的动作及动作的概率分布可以不同, 因此也构成一个空间</p>
<p>​			<strong>奖励 reward space</strong> - 在某状态, 执行某动作对应的奖励 (不是固定值, 而是概率分布)</p>
<p>​		<strong>2. 两个条件概率 : 状态转移的条件概率, 获得奖励的条件概率</strong>。</p>
<p>​			状态转移的条件概率：在状态S的情况且采取动作a，到达状态S‘的概率。</p>
<p>​			获得奖励的条件概率：在状态S的情况且采取动作a，得到奖励r的概率。</p>
<p>​		<strong>3. 策略:</strong> 在状态s 采取行动a的概率是多少</p>
<p>​		<strong>4. markov属性</strong>: 历史无关属性 (无后效性):<strong>意思就是说：下一阶段的state以及下一阶段所获得的reward，与之前的action和state无关</strong><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619153739063.png" alt="image-20240619153739063"></p>
<h3 id="——第二节课（p1）"><a href="#——第二节课（p1）" class="headerlink" title="——第二节课（p1）"></a>——第二节课（p1）</h3><h4 id="1-举例"><a href="#1-举例" class="headerlink" title="1.举例"></a>1.举例</h4><p>​	<strong>return的重要性：我们可以通过比较policy的return来判断一个policy的好坏</strong></p>
<p>​		策略1：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162401938.png" alt="image-20240619162401938"></p>
<p>​	 <strong>从S1到S3 reward&#x3D;0，S3到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162438168.png" alt="image-20240619162438168"></p>
<p>​	  	策略2：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162741338.png" alt="image-20240619162741338"></p>
<p>​		<strong>从S1到S2 reward&#x3D;-1，S2到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162846264.png" alt="image-20240619162846264"></p>
<p>​			策略3：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163011002.png" alt="image-20240619163011002"></p>
<p>​		<strong>①：50%的概率 从S1到S3 reward&#x3D;0，S3到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong></p>
<p>​		<strong>②：50%的概率 从S1到S2 reward&#x3D;-1，S2到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163222662.png" alt="image-20240619163222662"></p>
<p>​						<u>**第三个策略实际上已经不是return，原因是return是对一个轨迹而言的 **</u></p>
<h4 id="2-如何计算return"><a href="#2-如何计算return" class="headerlink" title="2.如何计算return"></a>2.如何计算return</h4><p>​                                        	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163944486.png" alt="image-20240619163944486"></p>
<p>方法一： (通过定义计算从不同的S出发而得到的return )</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619164158443.png" alt="image-20240619164158443"></p>
<p>​	方法二：在不同状态下所得到的return，他是依赖于从其它状态所得到的return。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619170916628.png" alt="image-20240619170916628"></p>
<p>​							    	<strong><u>方法二的return是在方法一的基础之上迭代出来的</u></strong>		</p>
<h4 id="3-贝尔曼公式的推导（特殊性）"><a href="#3-贝尔曼公式的推导（特殊性）" class="headerlink" title="3.贝尔曼公式的推导（特殊性）"></a>3.贝尔曼公式的推导（特殊性）</h4><p>​		利用    <strong>方法二</strong>    所得到的四个等式进行线性变化：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191913289.png" alt="image-20240619191913289"></p>
<p>​	    最后得到贝尔曼公式：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191956982.png" alt="image-20240619191956982"></p>
<h3 id="——第二节课（p2）"><a href="#——第二节课（p2）" class="headerlink" title="——第二节课（p2）"></a>——第二节课（p2）</h3><h4 id="1-state-value"><a href="#1-state-value" class="headerlink" title="1.state value"></a>1.state value</h4><p>​		state value 全称 state-value fuction</p>
<ul>
<li><p>**<u>state value是对从某一状态状态出发 多条trajectory的return的期望（或者是平均或者是mean）</u>**【公式写的很唬人，但是并没有】</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619193816799.png" alt="image-20240619193816799"></p>
<p>​		①：return 一个是确定的统计值，但是state value是一个不确定的预估值</p>
<p>​		②：V是多个轨迹的平均值（期望值），G是一个轨迹的汇报，一个状态可能有多个轨迹</p>
<p>​		③：state value中value的数值比较大，应该表示从<strong>这个 state 出发的 policy 更有价值</strong>，而不是 状态 更有价值。【然后从这个state 并且利用这个policy出发我会获得更多的return】</p>
<p>​		④：return是一条轨迹的reward之和，state value是某个位置的Gt的期望，这个state value是和policy有关的，各个action概率x对应状态Gt之和。</p>
</li>
<li><p><u><strong>return与state value的区别</strong></u></p>
</li>
</ul>
<p>​		区别：return是针对单个trajectory所求的return，而state value 是对多个trajectory 我所得到的return 然后再求的期望。<strong>【就是因为从一个状态出发具有不确定性，所以能够得到多条trajectory】</strong></p>
<h4 id="2-举例"><a href="#2-举例" class="headerlink" title="2.举例"></a>2.举例</h4><p>​	第一个图采取策略1，第二个图采取策略2，第三个图采取策略3。<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195156253.png" alt="image-20240619195156253"></p>
<p>​	由于策略1与策略2只有一条trajectory，因此state value &#x3D; return</p>
<p>​	策略2有两条trajectory，因为state value是两条trajectory 得到return的一个平均值。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195751413.png" alt="image-20240619195751413"></p>
<h3 id="——第二节课（p3）"><a href="#——第二节课（p3）" class="headerlink" title="——第二节课（p3）"></a>——第二节课（p3）</h3><h4 id="1-贝尔曼公式普遍性推导"><a href="#1-贝尔曼公式普遍性推导" class="headerlink" title="1.贝尔曼公式普遍性推导"></a>1.贝尔曼公式普遍性推导</h4><p>​	<u>计算 state value 的工具就是 贝尔曼公式</u>：<strong>描述了不同状态state value之间的关系</strong></p>
<p>​		<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622202846552.png" alt="image-20240622202846552"></p>
<ul>
<li><p>对<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203018759.png" alt="image-20240622203018759">的证明：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203248744.png" alt="image-20240622203248744"></p>
</li>
</ul>
<p>​			需要注意的点：这个其实就是我们立即得到的奖励。<strong>（因此此时Rt+1&#x3D;+Rt）</strong></p>
<ul>
<li><p>对<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203509509.png" alt="image-20240622203509509">的证明：</p>
<p>​							    	 	  <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622204420356.png" alt="image-20240622204420356"></p>
</li>
</ul>
<p>​			需要注意的点：这个其实就是我们未来得到的奖励。	</p>
<p>最终得到的结果: 	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622211359926.png" alt="image-20240622211359926">		  </p>
<p>贝尔曼公式的优点：</p>
<p>​				①：可以描述不同状态的state value之前的关系。<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214605992.png" alt="image-20240622214605992">与<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214645269.png" alt="image-20240622214645269">之间的关系。</p>
<p>​				②：这不仅仅是一个式子，对于所有的S 都可以实现。</p>
<p>贝尔公式的本质：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715104533466.png" alt="image-20240715104533466"></p>
<p>​	<strong>我们可以通过贝尔曼公式快速的将各个位置的state value计算出来：</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715105918534.png" alt="image-20240715105918534"></p>
<p>四个未知数，四个方程求解每个状态非常简单直接给出答案。（state value 只和 y 有关）</p>
<p>​                            <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110206450.png" alt="image-20240715110206450">					</p>
<p>倘若我们设置一个γ&#x3D;0.9，则我们可以计算出各个状态的state value值。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110912989.png" alt="image-20240715110912989"></p>
<p>​			**<u>之前我们一直强调state value 代表着状态的价值，state value 越高。说明我们越要往那个地方走</u>**。根据计算和图标我们可以知道，S2，S3，S4都离目的地           <strong><u>更加的近</u></strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715111357867.png" alt="image-20240715111357867"></p>
<p>在有两个方向可以走的话，state value 要乘以相对应的概率。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155354938.png" alt="image-20240715155354938"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155420097.png" alt="image-20240715155420097"></p>
<p><strong>其他状态的state value 保持不变</strong></p>
<h3 id="——第二节课（p4）"><a href="#——第二节课（p4）" class="headerlink" title="——第二节课（p4）"></a>——第二节课（p4）</h3><h4 id="1-贝尔曼矩阵向量形式（matrix-vector-form）的推导"><a href="#1-贝尔曼矩阵向量形式（matrix-vector-form）的推导" class="headerlink" title="1.贝尔曼矩阵向量形式（matrix-vector form）的推导"></a>1.贝尔曼矩阵向量形式（matrix-vector form）的推导</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170156816.png" alt="image-20240715170156816"></p>
<p>​		<strong>将①、②结合：</strong><u>表示从当前状态出发所得到的immediate reward 的期望（也就是所谓的平均值）</u></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161525151.png" alt="image-20240715161525151"></p>
<p>​		<strong>将①、③结合：</strong><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161737910.png" alt="image-20240715161737910"></p>
<p>​		最后得到的结果：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170605152.png" alt="image-20240715170605152"></p>
<p>​		再进行缩写：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171114197.png" alt="image-20240715171114197"></p>
<p>   <strong>PΠ即为所谓的状态状态转移矩阵:</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171321412.png" alt="image-20240715171321412"></p>
<h3 id="——第二节课（p5）"><a href="#——第二节课（p5）" class="headerlink" title="——第二节课（p5）"></a>——第二节课（p5）</h3><h4 id="1-state-value和action-value的区别"><a href="#1-state-value和action-value的区别" class="headerlink" title="1.state value和action value的区别"></a>1.state value和action value的区别</h4><ul>
<li>state value表示从一个状态出发然后可以获得的期望return，而action value表示从一个状态出发<strong>并且</strong>采取相应的action然后可以获得的期望return。</li>
</ul>
<h4 id="2-action-value的定义"><a href="#2-action-value的定义" class="headerlink" title="2.action value的定义"></a>2.action value的定义</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240716160217245.png" alt="image-20240716160217245"></p>
<p>​	举例说明action value：</p>
<p>​        	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095247150.png" alt="image-20240717095247150"></p>
<p>​			<strong>在状态S1的前提下，采取action a2的action value ：</strong>（等式的左边写法类似于state value）</p>
<p>​						<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095738268.png" alt="image-20240717095738268"></p>
<ul>
<li><u>需要注意的点，尽管S1仅仅只往右走，但是不表示他<strong>其他方向</strong>的action value为0。</u></li>
</ul>
<h3 id="——第三节课（p1）"><a href="#——第三节课（p1）" class="headerlink" title="——第三节课（p1）"></a>——第三节课（p1）</h3><h4 id="1-通过例子引出”贝尔曼最优公式“"><a href="#1-通过例子引出”贝尔曼最优公式“" class="headerlink" title="1.通过例子引出”贝尔曼最优公式“"></a>1.通过例子引出”贝尔曼最优公式“</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150326386.png" alt="image-20240717150326386"></p>
<ul>
<li><p>先通过贝尔曼公式将每个状态的是state value 计算出来。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150448821.png" alt="image-20240717150448821"></p>
</li>
<li><p>然后将<strong>γ&#x3D;0.9</strong>代入上式将各个state value 计算出来</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150840806.png" alt="image-20240717150840806"></p>
</li>
<li><p>以S<sub>1</sub>为例，分别计算出他五个action（上、右、下、左、原地）的<strong>action value</strong>。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717153539867.png" alt="image-20240717153539867"></p>
</li>
</ul>
<p>​	**不难发现向下走的策略反而是好的，但是图中的S<sub>1</sub>**的动作确是向右的，因此我们需要不断改变，得到一个最优策略。</p>
<h3 id="——第三节课（p2）"><a href="#——第三节课（p2）" class="headerlink" title="——第三节课（p2）"></a>——第三节课（p2）</h3><h4 id="1-贝尔曼最优公式"><a href="#1-贝尔曼最优公式" class="headerlink" title="1.贝尔曼最优公式"></a>1.贝尔曼最优公式</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717203326321.png" alt="image-20240717203326321"></p>
<ul>
<li>相比于贝尔曼公式公式，前面只是多了一个<strong>max Π</strong></li>
</ul>
<h4 id="2-如何求解贝尔曼最优公式"><a href="#2-如何求解贝尔曼最优公式" class="headerlink" title="2.如何求解贝尔曼最优公式"></a>2.如何求解贝尔曼最优公式</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204558842.png" alt="image-20240717204558842"></p>
<ul>
<li>在①处的概率 在gird word中只有五种可能，（向上、向左、向下、向左、原地打圈）</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204857582.png" alt="image-20240717204857582"></p>
<p>倘若要求解贝尔曼最优公式，上述例子最好证明：</p>
<ul>
<li>假设q<sub>1</sub>与q<sub>2</sub>的值都小于q<sub>3</sub>的值，因此只有q<sub>3</sub>的概率为1的时候，整个式子的值最大</li>
<li>由于概率的合一性，c<sub>1</sub>、c<sub>2</sub>、c<sub>3</sub>的概率之和为1</li>
</ul>
<p><strong><u>因此：贝尔曼公式的最优解就是q（s，a）的最大值</u></strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717205846572.png" alt="image-20240717205846572"></p>
<h3 id="——第三节课（p3）"><a href="#——第三节课（p3）" class="headerlink" title="——第三节课（p3）"></a>——第三节课（p3）</h3><h4 id="1-将贝尔曼最优公式写成v-f-v"><a href="#1-将贝尔曼最优公式写成v-f-v" class="headerlink" title="1.将贝尔曼最优公式写成v&#x3D;f(v)"></a>1.将贝尔曼最优公式写成v&#x3D;f(v)</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104532551.png" alt="image-20240719104532551"></p>
<p>​	**对每一个v，都能求出一个max，这个max是和v有关的式子。那么把v看成变量，max就是v的函数了，把这个函数定义成f(v)**。因此贝尔曼最优公式则可以变成：</p>
<p>​                                         	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104614360.png" alt="image-20240719104614360"></p>
<h4 id="2-Contraction-mapping-theorem（收缩映射定理）"><a href="#2-Contraction-mapping-theorem（收缩映射定理）" class="headerlink" title="2.Contraction mapping theorem（收缩映射定理）"></a>2.Contraction mapping theorem（收缩映射定理）</h4><ul>
<li>Fixed point的概念：	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105138076.png" alt="image-20240719105138076"></li>
</ul>
<p>​	<strong>（自己的理解：其实就是y&#x3D;f（x）与y&#x3D;x的交点。）</strong></p>
<ul>
<li>Contraction mapping (收缩映射),满足以下不等式的函数可以称为具有<strong>收缩映射</strong></li>
</ul>
<p>​	    <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105829512.png" alt="image-20240719105829512"></p>
<ol>
<li>其中γ的范围属于（0，1）</li>
<li>翻译过来是函数值的差的绝对值 <strong>小于等于</strong> 自变量差的绝对值</li>
</ol>
<ul>
<li>如果一个f满足<strong>收缩映射定理</strong>，那么它会拥有以下三点性质。</li>
</ul>
<p>​		①：一定存在一个fixed point。</p>
<p>​		②：这个fixed point 是唯一存在。</p>
<p>​		③：可以通过迭代算法求解出fixed point。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以上说了那么多，我们只是去想证明贝尔曼最优公式具有“收缩映射定理”。</span><br></pre></td></tr></table></figure>

<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719153155744.png" alt="image-20240719153155744"></p>
<p>​	将f（v）带入一下的式子发现γ的值在0到1之间，则说明贝尔曼最优公式满足<strong>“收缩映射定理”</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719154439357.png" alt="image-20240719154439357"></p>
<p>​	<u>因此贝尔曼最优公式可以也具有“收缩映射定理”的三条性质。</u></p>
<ol>
<li>贝尔曼公式一定存在一个解。我们用V<sup>*</sup>进行表示</li>
<li>并且这个解是唯一的</li>
<li>并且这个解可以通过<strong>迭代的方式</strong>将解求解出来。</li>
</ol>
<h3 id="——第三节课（p4）"><a href="#——第三节课（p4）" class="headerlink" title="——第三节课（p4）"></a>——第三节课（p4）</h3><h4 id="1-影响贝尔曼最优公式的影响因素"><a href="#1-影响贝尔曼最优公式的影响因素" class="headerlink" title="1.影响贝尔曼最优公式的影响因素"></a>1.影响贝尔曼最优公式的影响因素</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719160609559.png" alt="image-20240719160609559"></p>
<ul>
<li>红色的量全为已知的量。也是影响最优解的因素。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719161205019.png" alt="image-20240719161205019"></p>
<ul>
<li>我们一般通过改变<strong>r（reward）和γ（rate）</strong>，来判断系统的策略会发生怎么样的。</li>
</ul>
<h4 id="2-通过修改“reward”和“γ”会改变最优策略"><a href="#2-通过修改“reward”和“γ”会改变最优策略" class="headerlink" title="2.通过修改“reward”和“γ”会改变最优策略"></a>2.通过修改“reward”和“γ”会改变最优策略</h4><ul>
<li>γ&#x3D;0.9时：</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162237012.png" alt="image-20240719162237012"></p>
<ul>
<li>γ从0.9到0.5时：</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162344758.png" alt="image-20240719162344758"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里有个细节：当γ比较大的时候，agent会比较远视，它会重视未来的reward</span><br><span class="line">           当γ比较小的时候，agent会比较远=近视，它会重视现在的reward</span><br></pre></td></tr></table></figure>

<h4 id="3-最优策略的不变性"><a href="#3-最优策略的不变性" class="headerlink" title="3.最优策略的不变性"></a>3.最优策略的不变性</h4><p><u><strong>如果将reward进行线性变化，变成新的reward。那么最优的策略不会变化。</strong></u></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162848871.png" alt="image-20240719162848871"></p>
<p>​						</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/07/24/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/07/25/pytorch%E5%AE%89%E8%A3%85jupyter%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/" rel="next" title="在pytorch环境中安装jupyter所遇到的问题">
      在pytorch环境中安装jupyter所遇到的问题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.1.</span> <span class="nav-text">——第一节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-agent"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.agent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-state"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.state</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-action"><span class="nav-number">1.1.3.</span> <span class="nav-text">3.action</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-state-transition"><span class="nav-number">1.1.4.</span> <span class="nav-text">4.state transition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-forbidden-area"><span class="nav-number">1.1.5.</span> <span class="nav-text">5.forbidden area</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-state-transition-probability"><span class="nav-number">1.1.6.</span> <span class="nav-text">6.state transition probability</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-policy"><span class="nav-number">1.1.7.</span> <span class="nav-text">7.policy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">——第一节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-reward"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.reward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-trajectory"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.trajectory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-return"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.return</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-discounted-return"><span class="nav-number">1.2.4.</span> <span class="nav-text">4.discounted return</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-episode"><span class="nav-number">1.2.5.</span> <span class="nav-text">5.episode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.3.</span> <span class="nav-text">——第二节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%B8%BE%E4%BE%8B"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.举例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97return"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.如何计算return</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E7%9A%84%E6%8E%A8%E5%AF%BC%EF%BC%88%E7%89%B9%E6%AE%8A%E6%80%A7%EF%BC%89"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.贝尔曼公式的推导（特殊性）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.4.</span> <span class="nav-text">——第二节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-state-value"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.state value</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BE%E4%BE%8B"><span class="nav-number">1.4.2.</span> <span class="nav-text">2.举例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.5.</span> <span class="nav-text">——第二节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E6%99%AE%E9%81%8D%E6%80%A7%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.5.1.</span> <span class="nav-text">1.贝尔曼公式普遍性推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p4%EF%BC%89"><span class="nav-number">1.6.</span> <span class="nav-text">——第二节课（p4）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E5%BD%A2%E5%BC%8F%EF%BC%88matrix-vector-form%EF%BC%89%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.6.1.</span> <span class="nav-text">1.贝尔曼矩阵向量形式（matrix-vector form）的推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p5%EF%BC%89"><span class="nav-number">1.7.</span> <span class="nav-text">——第二节课（p5）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-state-value%E5%92%8Caction-value%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.7.1.</span> <span class="nav-text">1.state value和action value的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-action-value%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">1.7.2.</span> <span class="nav-text">2.action value的定义</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.8.</span> <span class="nav-text">——第三节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E5%BC%95%E5%87%BA%E2%80%9D%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%E2%80%9C"><span class="nav-number">1.8.1.</span> <span class="nav-text">1.通过例子引出”贝尔曼最优公式“</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.9.</span> <span class="nav-text">——第三节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="nav-number">1.9.1.</span> <span class="nav-text">1.贝尔曼最优公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="nav-number">1.9.2.</span> <span class="nav-text">2.如何求解贝尔曼最优公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.10.</span> <span class="nav-text">——第三节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%B0%86%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%E5%86%99%E6%88%90v-f-v"><span class="nav-number">1.10.1.</span> <span class="nav-text">1.将贝尔曼最优公式写成v&#x3D;f(v)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Contraction-mapping-theorem%EF%BC%88%E6%94%B6%E7%BC%A9%E6%98%A0%E5%B0%84%E5%AE%9A%E7%90%86%EF%BC%89"><span class="nav-number">1.10.2.</span> <span class="nav-text">2.Contraction mapping theorem（收缩映射定理）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p4%EF%BC%89"><span class="nav-number">1.11.</span> <span class="nav-text">——第三节课（p4）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%BD%B1%E5%93%8D%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0"><span class="nav-number">1.11.1.</span> <span class="nav-text">1.影响贝尔曼最优公式的影响因素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%80%9A%E8%BF%87%E4%BF%AE%E6%94%B9%E2%80%9Creward%E2%80%9D%E5%92%8C%E2%80%9C%CE%B3%E2%80%9D%E4%BC%9A%E6%94%B9%E5%8F%98%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="nav-number">1.11.2.</span> <span class="nav-text">2.通过修改“reward”和“γ”会改变最优策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5%E7%9A%84%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="nav-number">1.11.3.</span> <span class="nav-text">3.最优策略的不变性</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ShaoXin Cui</p>
  <div class="site-description" itemprop="description">有志者事竟成</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ShaoXin Cui</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
