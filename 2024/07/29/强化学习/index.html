<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="强化学习强化学习的终极目标：求解最优策略 ——第一节课（p1）1.agent​	基本上就是用于决策的模型整体 2.state​	 当前的状态, 以平面迷宫为例, 就是当前所处的格子。 ​		1. Stata space: 所有状态的集合。（ i 的范围从 1—9）  3.action​	可采取的行动, 还是以平面迷宫为例, 有上下左右移动或者原地不动5个action。 4.state transi">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记">
<meta property="og:url" content="http://example.com/2024/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="CuiShao Xin&#39;s Blog">
<meta property="og:description" content="强化学习强化学习的终极目标：求解最优策略 ——第一节课（p1）1.agent​	基本上就是用于决策的模型整体 2.state​	 当前的状态, 以平面迷宫为例, 就是当前所处的格子。 ​		1. Stata space: 所有状态的集合。（ i 的范围从 1—9）  3.action​	可采取的行动, 还是以平面迷宫为例, 有上下左右移动或者原地不动5个action。 4.state transi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617145748004.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617153118346.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154110728.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154611741.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202537238.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202552718.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203241653.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203319922.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211131500.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211148758.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619153739063.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162401938.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162438168.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162741338.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162846264.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163011002.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163222662.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163944486.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619164158443.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619170916628.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191913289.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191956982.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619193816799.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195156253.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195751413.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622202846552.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203018759.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203248744.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203509509.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622204420356.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622211359926.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214605992.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214645269.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715104533466.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715105918534.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110206450.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110912989.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715111357867.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155354938.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155420097.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170156816.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161525151.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161737910.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170605152.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171114197.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171321412.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240716160217245.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095247150.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095738268.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150326386.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150448821.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150840806.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717153539867.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717203326321.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204558842.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204857582.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717205846572.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104532551.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104614360.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105138076.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105829512.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719153155744.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719154439357.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719160609559.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719161205019.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162237012.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162344758.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162848871.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240727165222742.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240727165623542.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729111932094.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729112205979.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145336610.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145520333.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145726173.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145910335.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729150140946.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729150314684.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729151504636.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729151656147.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729153558573.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154039737.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154101465.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154153639.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154202101.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154413699.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154425877.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729204941056.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729205206319.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729205639055.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730170009613.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730170038330.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730170948048.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730193940153.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194700181.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194620142.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194744298.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194819490.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194853095.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730195041697.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802155601658.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802160026036.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802204112509.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802211049997.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805153606163.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805153927936.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805154020741.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805184119814.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240821165316711.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240821161808433.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240821161745086.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240912110457828.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005102825893.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005103214111.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005103533830.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005103706760.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005154944756.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005155918960.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005160014555.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005160100252.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005161632677.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005161655914.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164136364.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164318206.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164428362.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164720680.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164808120.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005200630237.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005202127699.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005202220565.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005210245699.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007162537278.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007162739098.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007163244515.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007193734159.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007204638567.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007205121874.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021201410489.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021205232188.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021205404138.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021210142657.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021211458012.png">
<meta property="og:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021213016099.png">
<meta property="article:published_time" content="2024-07-29T13:50:30.076Z">
<meta property="article:modified_time" content="2024-10-22T13:08:20.744Z">
<meta property="article:author" content="ShaoXin Cui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617145748004.png">

<link rel="canonical" href="http://example.com/2024/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习笔记 | CuiShao Xin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">CuiShao Xin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ShaoXin Cui">
      <meta itemprop="description" content="有志者事竟成">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CuiShao Xin's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-07-29 21:50:30" itemprop="dateCreated datePublished" datetime="2024-07-29T21:50:30+08:00">2024-07-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-22 21:08:20" itemprop="dateModified" datetime="2024-10-22T21:08:20+08:00">2024-10-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p><u>强化学习的终极目标：求解最优策略</u></p>
<h3 id="——第一节课（p1）"><a href="#——第一节课（p1）" class="headerlink" title="——第一节课（p1）"></a>——第一节课（p1）</h3><h4 id="1-agent"><a href="#1-agent" class="headerlink" title="1.agent"></a>1.agent</h4><p>​	基本上就是用于决策的模型整体</p>
<h4 id="2-state"><a href="#2-state" class="headerlink" title="2.state"></a><em>2</em>.state</h4><p>​	 当前的状态, 以平面迷宫为例, 就是当前所处的格子。</p>
<p>​		<em>1.</em> Stata space: 所有状态的集合。（ i 的范围从 1—9）</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617145748004.png" alt="image-20240617145748004"></p>
<h4 id="3-action"><a href="#3-action" class="headerlink" title="3.action"></a>3.action</h4><p>​	可采取的行动, 还是以平面迷宫为例, 有上下左右移动或者原地不动5个action。</p>
<h4 id="4-state-transition"><a href="#4-state-transition" class="headerlink" title="4.state transition"></a>4.state transition</h4><p>​	从某一个state到另外一个state。</p>
<h4 id="5-forbidden-area"><a href="#5-forbidden-area" class="headerlink" title="5.forbidden area"></a>5.forbidden area</h4><ul>
<li>情况一：可以进入但是会受到惩罚。（我们考虑的是这种情况）</li>
<li>情况二：不可以进入。</li>
</ul>
<h4 id="6-state-transition-probability"><a href="#6-state-transition-probability" class="headerlink" title="6.state transition probability"></a>6.state transition probability</h4><ul>
<li><p>白话文：我们此时处于S1状态，如果我们选择向右走，则下一个状态则是S2。</p>
<p>​	在数学中，在状态为S1和下一步向右走，得到S2的状态概率为1。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617153118346.png" alt="image-20240617153118346"></p>
</li>
</ul>
<h4 id="7-policy"><a href="#7-policy" class="headerlink" title="7.policy"></a>7.policy</h4><p>​	告诉agent处于某一个状态下，应该做出怎样的action。</p>
<ul>
<li><p>Π相当于是一个策略，他会告诉我们处于这个state下，各个action的<em>概率</em>。<strong>（这是个确定的情况）</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154110728.png" alt="image-20240617154110728"></p>
</li>
<li><p><strong>（不确定的情况）</strong></p>
</li>
</ul>
<p>​                                                                                                    <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617154611741.png" alt="image-20240617154611741">  </p>
<h3 id="——第一节课（p2）"><a href="#——第一节课（p2）" class="headerlink" title="——第一节课（p2）"></a>——第一节课（p2）</h3><h4 id="1-reward"><a href="#1-reward" class="headerlink" title="1.reward"></a>1.reward</h4><ul>
<li><p>正数：<strong>在采取某一个动作之后</strong>，会获得奖励。</p>
</li>
<li><p>负数：<strong>在采取某一个动作之后</strong>，会获得惩罚。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202537238.png" alt="image-20240617202537238"></p>
</li>
</ul>
<p>​		①在状态S1的情况下，如果我们选择向上走，则奖励会是-1。</p>
<p>​		②在概率模型下，如果状态S1的情况下，我们选择向上走，则得到奖励是-1的概率是1。 </p>
<p>​			<strong>（reward一定是依赖于当前的state和action，而不是依赖与下一状态。）</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617202552718.png" alt="image-20240617202552718"></p>
<h4 id="2-trajectory"><a href="#2-trajectory" class="headerlink" title="2.trajectory"></a>2.trajectory</h4><p>​	一系列状态(state)和行为(action)交替的轨迹, 类似一条线路。                                                                  <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203241653.png" alt="image-20240617203241653">	</p>
<p>​			红色的线即为<strong>trajectory</strong>。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617203319922.png" alt="image-20240617203319922"></p>
<h4 id="3-return"><a href="#3-return" class="headerlink" title="3.return"></a>3.return</h4><p>​	<u><strong>return是对于trajectory而言的。</strong></u></p>
<p>​	对于上图的trajectory而言，return&#x3D;0+0+0+1&#x3D;1。</p>
<p>​	<strong>return的作用：我们可以通过return来判断一个policy与另外一个policy哪一个更好</strong>。</p>
<h4 id="4-discounted-return"><a href="#4-discounted-return" class="headerlink" title="4.discounted return"></a>4.discounted return</h4><p>​	说到底就是discount rate和return之间的结合。首先这个discount rate ：η是一个大于0小于1 的数。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211131500.png" alt="image-20240617211131500"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240617211148758.png" alt="image-20240617211148758"></p>
<p>其中上图中的0，1<strong>都是reward</strong>。</p>
<p><u>引入discounted return的作用</u></p>
<ul>
<li>在trajectory<strong>为无限的情况下</strong>，所得的return为一个不收敛的值。加上discount rate之后现在是一个有限的值。</li>
<li>如果η趋近于0，则agent会越来越注重  <strong>现在</strong>  的reward。</li>
<li>如果η趋近于1，则agent会越来越注重  <strong>未来</strong>  的reward。</li>
</ul>
<h4 id="5-episode"><a href="#5-episode" class="headerlink" title="5.episode"></a>5.episode</h4><p>​	episode是带有<strong>terminal states（最终状态）</strong>的trajectory。</p>
<p>​			episodicTasks : 带有终止状态的任务。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>​	在MDP（马尔科夫决策过程）框架之下：	</p>
<p>​		<strong>1.要素：</strong></p>
<p>​			 <strong>状态 state space</strong> - 可以直观的理解为地图</p>
<p>​    		 地图本身就是一个空间(space)</p>
<p>​			<strong>动作 action space</strong> - 上下左右以及原地不动 (不是固定值, 而是概率分布)</p>
<p>​			不同状态(state) , 能够采取的动作及动作的概率分布可以不同, 因此也构成一个空间</p>
<p>​			<strong>奖励 reward space</strong> - 在某状态, 执行某动作对应的奖励 (不是固定值, 而是概率分布)</p>
<p>​		<strong>2. 两个条件概率 : 状态转移的条件概率, 获得奖励的条件概率</strong>。</p>
<p>​			状态转移的条件概率：在状态S的情况且采取动作a，到达状态S‘的概率。</p>
<p>​			获得奖励的条件概率：在状态S的情况且采取动作a，得到奖励r的概率。</p>
<p>​		<strong>3. 策略:</strong> 在状态s 采取行动a的概率是多少</p>
<p>​		<strong>4. markov属性</strong>: 历史无关属性 (无后效性):<strong>意思就是说：下一阶段的state以及下一阶段所获得的reward，与之前的action和state无关</strong><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619153739063.png" alt="image-20240619153739063"></p>
<h3 id="——第二节课（p1）"><a href="#——第二节课（p1）" class="headerlink" title="——第二节课（p1）"></a>——第二节课（p1）</h3><h4 id="1-举例"><a href="#1-举例" class="headerlink" title="1.举例"></a>1.举例</h4><p>​	<strong>return的重要性：我们可以通过比较policy的return来判断一个policy的好坏</strong></p>
<p>​		策略1：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162401938.png" alt="image-20240619162401938"></p>
<p>​	 <strong>从S1到S3 reward&#x3D;0，S3到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162438168.png" alt="image-20240619162438168"></p>
<p>​	  	策略2：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162741338.png" alt="image-20240619162741338"></p>
<p>​		<strong>从S1到S2 reward&#x3D;-1，S2到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619162846264.png" alt="image-20240619162846264"></p>
<p>​			策略3：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163011002.png" alt="image-20240619163011002"></p>
<p>​		<strong>①：50%的概率 从S1到S3 reward&#x3D;0，S3到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong></p>
<p>​		<strong>②：50%的概率 从S1到S2 reward&#x3D;-1，S2到S4 reward&#x3D;1，然后一直待在S4里面reward一直等于1。</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163222662.png" alt="image-20240619163222662"></p>
<p>​						<u>**第三个策略实际上已经不是return，原因是return是对一个轨迹而言的 **</u></p>
<h4 id="2-如何计算return"><a href="#2-如何计算return" class="headerlink" title="2.如何计算return"></a>2.如何计算return</h4><p>​                                        	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619163944486.png" alt="image-20240619163944486"></p>
<p>方法一： (通过定义计算从不同的S出发而得到的return )</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619164158443.png" alt="image-20240619164158443"></p>
<p>​	方法二：在不同状态下所得到的return，他是依赖于从其它状态所得到的return。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619170916628.png" alt="image-20240619170916628"></p>
<p>​							    	<strong><u>方法二的return是在方法一的基础之上迭代出来的</u></strong>		</p>
<h4 id="3-贝尔曼公式的推导（特殊性）"><a href="#3-贝尔曼公式的推导（特殊性）" class="headerlink" title="3.贝尔曼公式的推导（特殊性）"></a>3.贝尔曼公式的推导（特殊性）</h4><p>​		利用    <strong>方法二</strong>    所得到的四个等式进行线性变化：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191913289.png" alt="image-20240619191913289"></p>
<p>​	    最后得到贝尔曼公式：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619191956982.png" alt="image-20240619191956982"></p>
<h3 id="——第二节课（p2）"><a href="#——第二节课（p2）" class="headerlink" title="——第二节课（p2）"></a>——第二节课（p2）</h3><h4 id="1-state-value"><a href="#1-state-value" class="headerlink" title="1.state value"></a>1.state value</h4><p>​		state value 全称 state-value fuction</p>
<ul>
<li><p>**<u>state value是对从某一状态状态出发 多条trajectory的return的期望（或者是平均或者是mean）</u>**【公式写的很唬人，但是并没有】</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619193816799.png" alt="image-20240619193816799"></p>
<p>​		①：return 一个是确定的统计值，但是state value是一个不确定的预估值</p>
<p>​		②：V是多个轨迹的平均值（期望值），G是一个轨迹的汇报，一个状态可能有多个轨迹</p>
<p>​		③：state value中value的数值比较大，应该表示从<strong>这个 state 出发的 policy 更有价值</strong>，而不是 状态 更有价值。【然后从这个state 并且利用这个policy出发我会获得更多的return】</p>
<p>​		④：return是一条轨迹的reward之和，state value是某个位置的Gt的期望，这个state value是和policy有关的，各个action概率x对应状态Gt之和。</p>
</li>
<li><p><u><strong>return与state value的区别</strong></u></p>
</li>
</ul>
<p>​		区别：return是针对单个trajectory所求的return，而state value 是对多个trajectory 我所得到的return 然后再求的期望。<strong>【就是因为从一个状态出发具有不确定性，所以能够得到多条trajectory】</strong></p>
<h4 id="2-举例"><a href="#2-举例" class="headerlink" title="2.举例"></a>2.举例</h4><p>​	第一个图采取策略1，第二个图采取策略2，第三个图采取策略3。<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195156253.png" alt="image-20240619195156253"></p>
<p>​	由于策略1与策略2只有一条trajectory，因此state value &#x3D; return</p>
<p>​	策略2有两条trajectory，因为state value是两条trajectory 得到return的一个平均值。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240619195751413.png" alt="image-20240619195751413"></p>
<h3 id="——第二节课（p3）"><a href="#——第二节课（p3）" class="headerlink" title="——第二节课（p3）"></a>——第二节课（p3）</h3><h4 id="1-贝尔曼公式普遍性推导"><a href="#1-贝尔曼公式普遍性推导" class="headerlink" title="1.贝尔曼公式普遍性推导"></a>1.贝尔曼公式普遍性推导</h4><p>​	<u>计算 state value 的工具就是 贝尔曼公式</u>：<strong>描述了不同状态state value之间的关系</strong></p>
<p>​		<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622202846552.png" alt="image-20240622202846552"></p>
<ul>
<li><p>对<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203018759.png" alt="image-20240622203018759">的证明：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203248744.png" alt="image-20240622203248744"></p>
</li>
</ul>
<p>​			需要注意的点：这个其实就是我们立即得到的奖励。<strong>（因此此时Rt+1&#x3D;+Rt）</strong></p>
<ul>
<li><p>对<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622203509509.png" alt="image-20240622203509509">的证明：</p>
<p>​							    	 	  <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622204420356.png" alt="image-20240622204420356"></p>
</li>
</ul>
<p>​			需要注意的点：这个其实就是我们未来得到的奖励。	</p>
<p>最终得到的结果: 	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622211359926.png" alt="image-20240622211359926">		  </p>
<p>贝尔曼公式的优点：</p>
<p>​				①：可以描述<strong>不同状态</strong>的state value之前的关系。<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214605992.png" alt="image-20240622214605992">与<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240622214645269.png" alt="image-20240622214645269">之间的关系。【也可以是同一状态】</p>
<p>​				②：这不仅仅是一个式子，对于所有的S 都可以实现。</p>
<p>贝尔公式的本质：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715104533466.png" alt="image-20240715104533466"></p>
<p>​	<strong>我们可以通过贝尔曼公式快速的将各个位置的state value计算出来：</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715105918534.png" alt="image-20240715105918534"></p>
<p>四个未知数，四个方程求解每个状态非常简单直接给出答案。（state value 只和 y 有关）</p>
<p>​                                                             <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110206450.png" alt="image-20240715110206450">					</p>
<p>倘若我们设置一个γ&#x3D;0.9，则我们可以计算出各个状态的state value值。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715110912989.png" alt="image-20240715110912989"></p>
<p>​			**<u>之前我们一直强调state value 代表着状态的价值，state value 越高。说明我们越要往那个地方走</u>**。根据计算和图标我们可以知道，S2，S3，S4都离目的地           <strong><u>更加的近</u></strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715111357867.png" alt="image-20240715111357867"></p>
<p>在有两个方向可以走的话，state value 要乘以相对应的概率。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155354938.png" alt="image-20240715155354938"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715155420097.png" alt="image-20240715155420097"></p>
<p><strong>其他状态的state value 保持不变</strong></p>
<h3 id="——第二节课（p4）"><a href="#——第二节课（p4）" class="headerlink" title="——第二节课（p4）"></a>——第二节课（p4）</h3><h4 id="1-贝尔曼矩阵向量形式（matrix-vector-form）的推导"><a href="#1-贝尔曼矩阵向量形式（matrix-vector-form）的推导" class="headerlink" title="1.贝尔曼矩阵向量形式（matrix-vector form）的推导"></a>1.贝尔曼矩阵向量形式（matrix-vector form）的推导</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170156816.png" alt="image-20240715170156816"></p>
<p>​		<strong>将①、②结合：</strong><u>表示从当前状态出发所得到的immediate reward 的期望（也就是所谓的平均值）</u></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161525151.png" alt="image-20240715161525151"></p>
<p>​		<strong>将①、③结合：</strong><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715161737910.png" alt="image-20240715161737910"></p>
<p>​		最后得到的结果：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715170605152.png" alt="image-20240715170605152"></p>
<p>​		再进行缩写：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171114197.png" alt="image-20240715171114197"></p>
<p>   <strong>PΠ即为所谓的状态状态转移矩阵:</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240715171321412.png" alt="image-20240715171321412"></p>
<h3 id="——第二节课（p5）"><a href="#——第二节课（p5）" class="headerlink" title="——第二节课（p5）"></a>——第二节课（p5）</h3><h4 id="1-state-value和action-value的区别"><a href="#1-state-value和action-value的区别" class="headerlink" title="1.state value和action value的区别"></a>1.state value和action value的区别</h4><ul>
<li>state value表示从一个状态出发然后可以获得的期望return，而action value表示从一个状态出发<strong>并且</strong>采取相应的action然后可以获得的期望return。</li>
</ul>
<h4 id="2-action-value的定义"><a href="#2-action-value的定义" class="headerlink" title="2.action value的定义"></a>2.action value的定义</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240716160217245.png" alt="image-20240716160217245"></p>
<p>​	举例说明action value：</p>
<p>​                                                         	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095247150.png" alt="image-20240717095247150">···</p>
<p>​			<strong>在状态S1的前提下，采取action a2的action value ：</strong>（等式的左边写法类似于state value）</p>
<p>​					                                	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717095738268.png" alt="image-20240717095738268"></p>
<ul>
<li><u>需要注意的点，尽管S1仅仅只往右走，但是不表示他<strong>其他方向</strong>的action value为0。</u></li>
</ul>
<h3 id="——第三节课（p1）"><a href="#——第三节课（p1）" class="headerlink" title="——第三节课（p1）"></a>——第三节课（p1）</h3><h4 id="1-通过例子引出”贝尔曼最优公式“"><a href="#1-通过例子引出”贝尔曼最优公式“" class="headerlink" title="1.通过例子引出”贝尔曼最优公式“"></a>1.通过例子引出”贝尔曼最优公式“</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150326386.png" alt="image-20240717150326386"></p>
<ul>
<li><p>先通过贝尔曼公式将每个状态的是state value 计算出来。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150448821.png" alt="image-20240717150448821"></p>
</li>
<li><p>然后将<strong>γ&#x3D;0.9</strong>代入上式将各个state value 计算出来</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717150840806.png" alt="image-20240717150840806"></p>
</li>
<li><p>以S<sub>1</sub>为例，分别计算出他五个action（上、右、下、左、原地）的<strong>action value</strong>。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717153539867.png" alt="image-20240717153539867"></p>
</li>
</ul>
<p>​	**不难发现向下走的策略反而是好的，但是图中的S<sub>1</sub>**的动作确是向右的，因此我们需要不断改变，得到一个最优策略。</p>
<h3 id="——第三节课（p2）"><a href="#——第三节课（p2）" class="headerlink" title="——第三节课（p2）"></a>——第三节课（p2）</h3><h4 id="1-贝尔曼最优公式"><a href="#1-贝尔曼最优公式" class="headerlink" title="1.贝尔曼最优公式"></a>1.贝尔曼最优公式</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717203326321.png" alt="image-20240717203326321"></p>
<ul>
<li>相比于贝尔曼公式公式，前面只是多了一个<strong>max Π</strong></li>
</ul>
<h4 id="2-如何求解贝尔曼最优公式"><a href="#2-如何求解贝尔曼最优公式" class="headerlink" title="2.如何求解贝尔曼最优公式"></a>2.如何求解贝尔曼最优公式</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204558842.png" alt="image-20240717204558842"></p>
<ul>
<li>在①处的概率 在gird word中只有五种可能，（向上、向左、向下、向左、原地打圈）</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717204857582.png" alt="image-20240717204857582"></p>
<p>倘若要求解贝尔曼最优公式，上述例子最好证明：</p>
<ul>
<li>假设q<sub>1</sub>与q<sub>2</sub>的值都小于q<sub>3</sub>的值，因此只有q<sub>3</sub>的概率为1的时候，整个式子的值最大</li>
<li>由于概率的合一性，c<sub>1</sub>、c<sub>2</sub>、c<sub>3</sub>的概率之和为1</li>
</ul>
<p><strong><u>因此：贝尔曼公式的最优解就是q（s，a）的最大值</u></strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240717205846572.png" alt="image-20240717205846572"></p>
<h3 id="——第三节课（p3）"><a href="#——第三节课（p3）" class="headerlink" title="——第三节课（p3）"></a>——第三节课（p3）</h3><h4 id="1-将贝尔曼最优公式写成v-f-v"><a href="#1-将贝尔曼最优公式写成v-f-v" class="headerlink" title="1.将贝尔曼最优公式写成v&#x3D;f(v)"></a>1.将贝尔曼最优公式写成v&#x3D;f(v)</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104532551.png" alt="image-20240719104532551"></p>
<p>​	**对每一个v，都能求出一个max，这个max是和v有关的式子。那么把v看成变量，max就是v的函数了，把这个函数定义成f(v)**。因此贝尔曼最优公式则可以变成：</p>
<p>​                                                                                   	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719104614360.png" alt="image-20240719104614360"></p>
<h4 id="2-Contraction-mapping-theorem（收缩映射定理）"><a href="#2-Contraction-mapping-theorem（收缩映射定理）" class="headerlink" title="2.Contraction mapping theorem（收缩映射定理）"></a>2.Contraction mapping theorem（收缩映射定理）</h4><ul>
<li>Fixed point的概念：	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105138076.png" alt="image-20240719105138076"></li>
</ul>
<p>​	<strong>（自己的理解：其实就是y&#x3D;f（x）与y&#x3D;x的交点。）</strong></p>
<ul>
<li>Contraction mapping (收缩映射),满足以下不等式的函数可以称为具有<strong>收缩映射</strong></li>
</ul>
<p>​	    <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719105829512.png" alt="image-20240719105829512"></p>
<ol>
<li>其中γ的范围属于（0，1）</li>
<li>翻译过来是函数值的差的绝对值 <strong>小于等于</strong> 自变量差的绝对值</li>
</ol>
<ul>
<li>如果一个f满足<strong>收缩映射定理</strong>，那么它会拥有以下三点性质。</li>
</ul>
<p>​		①：一定存在一个fixed point。</p>
<p>​		②：这个fixed point 是唯一存在。</p>
<p>​		③：可以通过迭代算法求解出fixed point。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以上说了那么多，我们只是去想证明贝尔曼最优公式具有“收缩映射定理”。</span><br></pre></td></tr></table></figure>

<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719153155744.png" alt="image-20240719153155744"></p>
<p>​	将f（v）带入一下的式子发现γ的值在0到1之间，则说明贝尔曼最优公式满足<strong>“收缩映射定理”</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719154439357.png" alt="image-20240719154439357"></p>
<p>​	<u>因此贝尔曼最优公式可以也具有“收缩映射定理”的三条性质。</u></p>
<ol>
<li><p>贝尔曼公式一定存在一个解。我们用V<sup>*</sup>进行表示（策略所对应的最佳state value是唯一的，但是这个策略不一定是唯一的）</p>
</li>
<li><p>并且这个解是唯一的。</p>
</li>
<li><p>并且这个解可以通过<strong>迭代的方式</strong>将解求解出来。（X<sub>k+1</sub>&#x3D;f(X<sub>k</sub>)）</p>
</li>
</ol>
<h3 id="——第三节课（p4）"><a href="#——第三节课（p4）" class="headerlink" title="——第三节课（p4）"></a>——第三节课（p4）</h3><h4 id="1-影响贝尔曼最优公式的影响因素"><a href="#1-影响贝尔曼最优公式的影响因素" class="headerlink" title="1.影响贝尔曼最优公式的影响因素"></a>1.影响贝尔曼最优公式的影响因素</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719160609559.png" alt="image-20240719160609559"></p>
<ul>
<li>红色的量全为已知的量。也是影响最优解的因素。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719161205019.png" alt="image-20240719161205019"></p>
<ul>
<li>我们一般通过改变<strong>r（reward）和γ（rate）</strong>，来判断系统的策略会发生怎么样的。</li>
</ul>
<h4 id="2-通过修改“reward”和“γ”会改变最优策略"><a href="#2-通过修改“reward”和“γ”会改变最优策略" class="headerlink" title="2.通过修改“reward”和“γ”会改变最优策略"></a>2.通过修改“reward”和“γ”会改变最优策略</h4><ul>
<li>γ&#x3D;0.9时：</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162237012.png" alt="image-20240719162237012"></p>
<ul>
<li>γ从0.9到0.5时：</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162344758.png" alt="image-20240719162344758"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里有个细节：当γ比较大的时候，agent会比较远视，它会重视未来的reward</span><br><span class="line">           当γ比较小的时候，agent会比较远=近视，它会重视现在的reward</span><br></pre></td></tr></table></figure>

<h4 id="3-最优策略的不变性"><a href="#3-最优策略的不变性" class="headerlink" title="3.最优策略的不变性"></a>3.最优策略的不变性</h4><p><u><strong>如果将reward进行线性变化，变成新的reward。那么最优的策略不会变化。</strong></u></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240719162848871.png" alt="image-20240719162848871"></p>
<h3 id="——第四节课（p1）"><a href="#——第四节课（p1）" class="headerlink" title="——第四节课（p1）"></a>——第四节课（p1）</h3><h4 id="1-值迭代算法（Value-interation）"><a href="#1-值迭代算法（Value-interation）" class="headerlink" title="1.值迭代算法（Value interation）"></a>1.值迭代算法（Value interation）</h4><ul>
<li>贝尔曼最优公式：</li>
</ul>
<p>​	                  	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240727165222742.png" alt="image-20240727165222742"></p>
<ul>
<li><p>因为满足收<strong>缩映射定理</strong>，所以可以利用迭代进行计算。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240727165623542.png" alt="image-20240727165623542"></p>
</li>
</ul>
<h4 id="2-值迭代的步骤"><a href="#2-值迭代的步骤" class="headerlink" title="2.值迭代的步骤"></a>2.值迭代的步骤</h4><ul>
<li><p>第一步：Policy update</p>
<p>先求出下一步的最佳策略：先计算出每个状态下的action value ，此时最优的策略就是会选择最大的action value，也就是我们所说的Q<sub>k</sub>。下一步的最佳策略就是该Q<sub>k</sub>最大的action，这就是下一步的最佳策略，并且只要不是该action的，执行概率都是0。 </p>
</li>
<li><p>第二步：value update</p>
<p>计算下一步状态的state value我们就要借助<strong>缩映射定理的结论</strong></p>
<ul>
<li><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729111932094.png" alt="image-20240729111932094"></p>
</li>
<li><p>然后我们把<strong>最佳策略带入</strong>，此时最佳策略是该Q<sub>k</sub>最大的action，并且只要不是该action的，执行概率都是0。 所以得到的结果也应该是Q<sub>k</sub>最大的值。这个值就是新的state value。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729112205979.png" alt="image-20240729112205979"></p>
</li>
</ul>
</li>
</ul>
<p>​			</p>
<h4 id="3-举例说明"><a href="#3-举例说明" class="headerlink" title="3.举例说明"></a>3.举例说明</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145336610.png" alt="image-20240729145336610"></p>
<ul>
<li>先计算出每个状态下的【向上，向右，向下，向左，原地不动的】action value。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145520333.png" alt="image-20240729145520333"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145726173.png" alt="image-20240729145726173"></p>
<ul>
<li><p>对于每一个状态而言，最优的策略就是会选择最大的action value，也就是我们所说的Q<sub>k</sub>。<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729145910335.png" alt="image-20240729145910335"></p>
</li>
<li><p>然后我们把<strong>最佳策略带入</strong>，此时最佳策略是该Q<sub>k</sub>最大的action，并且只要不是该action的，执行概率都是0。 所以得到的结果也应该是Q<sub>k</sub>最大的值。这个值就是新的state value。</p>
</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729150140946.png" alt="image-20240729150140946"></p>
<p>因此，新的策略和新的state value的状态如下。也就是上图的图（b）。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729150314684.png" alt="image-20240729150314684"></p>
<p>然后不断的进行迭代可以得到新的策略，新的state value。<strong>一直到V<sub>k</sub>—V<sub>k+1</sub>是一个很小的数的时候停止迭代</strong>。</p>
<h3 id="——第四节课（p2）"><a href="#——第四节课（p2）" class="headerlink" title="——第四节课（p2）"></a>——第四节课（p2）</h3><h4 id="1-策略迭代算法（policy-evaluation）"><a href="#1-策略迭代算法（policy-evaluation）" class="headerlink" title="1.策略迭代算法（policy evaluation）"></a>1.策略迭代算法（policy evaluation）</h4><p>【策略迭代算法其实是基于价值迭代算法的一个结果】</p>
<ul>
<li>先任意给定一个策略Π，然后通过这个策略，利用贝尔曼公式得到相对应的state value【利用：<strong>贝尔曼公式</strong>】</li>
</ul>
<p>​	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729151504636.png" alt="image-20240729151504636"></p>
<ul>
<li>然后通过求解，action value进行获得下一步更好的策略。【利用：<strong>贝尔曼最优公式</strong>】</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729151656147.png" alt="image-20240729151656147"></p>
<h4 id="2-举例说明"><a href="#2-举例说明" class="headerlink" title="2.举例说明"></a>2.举例说明</h4><ul>
<li>初始的策略是如图（a）所示。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729153558573.png" alt="image-20240729153558573"></p>
<ul>
<li>先通过贝尔曼公式去计算每个状态的state value。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154039737.png" alt="image-20240729154039737"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154101465.png" alt="image-20240729154101465"></p>
<ul>
<li><p>倘若数据复杂的话，则得使用<strong>迭代的算法计算state value。</strong></p>
</li>
<li><p>然后计算每个状态的action value，然后做出比较并且选出新的策略。</p>
</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154153639.png" alt="image-20240729154153639"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154202101.png" alt="image-20240729154202101"></p>
<ul>
<li>通过计算计算出每个状态下最优的策略。然后将策略进行更新。（如图（b）所示）</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154413699.png" alt="image-20240729154413699"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729154425877.png" alt="image-20240729154425877"></p>
<h3 id="——第四节课（p3）"><a href="#——第四节课（p3）" class="headerlink" title="——第四节课（p3）"></a>——第四节课（p3）</h3><h4 id="1-策略迭代算法与价值迭代算法的区别"><a href="#1-策略迭代算法与价值迭代算法的区别" class="headerlink" title="1.策略迭代算法与价值迭代算法的区别"></a>1.策略迭代算法与价值迭代算法的区别</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729204941056.png" alt="image-20240729204941056"></p>
<ul>
<li><p>值迭代算法中, 这一步仅迭代一次,而策略迭代算法中这里要反复无限次的迭代, 直到找出稳定的state value, 再进行下一步。</p>
<ul>
<li><p>无限次迭代的含义：（简单的话可以直接计算）</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729205206319.png" alt="image-20240729205206319"></p>
</li>
</ul>
</li>
<li><p>策略迭代<strong>尽管在策略评估中可能需要进行多次迭代</strong>，但其通过策略改进步骤的逐步提升，<strong>能够更快地收敛到最优策略。</strong>而价值迭代需要在每次更新中逐步调整所有状态的价值函数，通常需要更多的迭代次数来达到收敛。</p>
</li>
</ul>
<h4 id="2-截断策略迭代（Iteration-of-truncation-strategies）"><a href="#2-截断策略迭代（Iteration-of-truncation-strategies）" class="headerlink" title="2.截断策略迭代（Iteration of truncation strategies）"></a>2.截断策略迭代（Iteration of truncation strategies）</h4><ul>
<li>定义：就是基于”策略迭代算法”与”价值迭代算法”中迭代次数取上一个中间的值。（比如说：我只计算<strong>j</strong>次）不需要向策略迭代算法一样迭代无数次。state value迭代到 j 次也就是vj时，不管有没有收敛都直接到下一步。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240729205639055.png" alt="image-20240729205639055"></p>
<h3 id="——第五节课（p1）"><a href="#——第五节课（p1）" class="headerlink" title="——第五节课（p1）"></a>——第五节课（p1）</h3><h4 id="1-通过例子介绍蒙特卡洛"><a href="#1-通过例子介绍蒙特卡洛" class="headerlink" title="1.通过例子介绍蒙特卡洛"></a>1.通过例子介绍蒙特卡洛</h4><p>​	<strong>model-based</strong>到<strong>model-free</strong>最重要的是通过<strong>蒙特卡洛估计</strong>。并且通过<strong>大数定律</strong>（law of large number）来估计<strong>概率</strong>。</p>
<ul>
<li><p><strong>model-Based思想:</strong></p>
<p>在之前讲到的model-Base 的算法中, 我们将一些提前准备好的状态空间, 奖励或动作的概率分布直接存储在模型(model)中, 因此我们能够精确的计算它们的数学期望,例如:50%+1.0reward 50%-1.0reward 时, 直接计算奖励的数学期望就是0.5<em>1.0 + 0.5</em>(-1.0) &#x3D; 0.0。</p>
</li>
<li><p><strong>model-Free思想:</strong></p>
</li>
</ul>
<p>​		model-Free意味着我们不再拥有这些信息, 只能通过多次实验取平均值的方式近似的估计期望。</p>
<h3 id="——第五节课（p2）"><a href="#——第五节课（p2）" class="headerlink" title="——第五节课（p2）"></a>——第五节课（p2）</h3><h4 id="1-MC-basic-算法（赵老师自己写出的算法，便于理解的）"><a href="#1-MC-basic-算法（赵老师自己写出的算法，便于理解的）" class="headerlink" title="1.MC basic 算法（赵老师自己写出的算法，便于理解的）"></a>1.MC basic 算法（赵老师自己写出的算法，便于理解的）</h4><p>​	 将<strong>策略迭代算法</strong>中**”策略评估”** 的部分, 由原来基于模型直接计算, 替换成基于多次实验采样来计算, 就得到了MC basic，所以MC basic的过程几乎与策略迭代算法相同。</p>
<ul>
<li><p>并且action value本来就有两种计算方法，一种基于模型直接计算的，另外一种基于多次实验采样来计算。</p>
<ul>
<li><p>基于模型直接计算的：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730170009613.png" alt="image-20240730170009613"></p>
</li>
<li><p>基于多次实验采样的：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730170038330.png" alt="image-20240730170038330"></p>
</li>
</ul>
</li>
</ul>
<h4 id="2-求解MC-basic的步骤"><a href="#2-求解MC-basic的步骤" class="headerlink" title="2.求解MC basic的步骤"></a>2.求解MC basic的步骤</h4><ul>
<li><p>首先从一个状态中的一个动作出发, 进行N次实验(episode)来收集数据。<strong>（episode的长度即为：并不是指迭代次数，可以将其理解为探索步数，当所有状态都可以在探索步数内到达目标点时策略就已经最优了）</strong></p>
<ul>
<li>比如在状态s1 , 我们多次从动作a1开始行动 并累计得到的结果(计算 discount return)</li>
</ul>
</li>
<li><p>最后把这些结果取平均, 来代替原本依赖模型计算出的的结果。（这些数据就是<strong>“经验”</strong>）</p>
</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730170948048.png" alt="image-20240730170948048"></p>
<ul>
<li><p>对于每一个state, 求出它能进行的所有动作(action)的行动价值(action value)</p>
<p>选择行动价值(action value)最大的动作(action), 更新策略。</p>
</li>
</ul>
<h3 id="——第五节课（p3）"><a href="#——第五节课（p3）" class="headerlink" title="——第五节课（p3）"></a>——第五节课（p3）</h3><h4 id="1-MC-Basic的例子"><a href="#1-MC-Basic的例子" class="headerlink" title="1.MC Basic的例子"></a>1.MC Basic的例子</h4><p>​	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730193940153.png" alt="image-20240730193940153"></p>
<ul>
<li><p>先给定特定的策略，然后通过action value的定义进行计算。</p>
<ul>
<li>（S1，a1）<ul>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194700181.png" alt="image-20240730194700181"></li>
</ul>
</li>
<li>（S1，a2）<ul>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194620142.png" alt="image-20240730194620142"></li>
</ul>
</li>
<li>（S1，a3）<ul>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194744298.png" alt="image-20240730194744298"></li>
</ul>
</li>
<li>（S1，a4）<ul>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194819490.png" alt="image-20240730194819490"></li>
</ul>
</li>
<li>（S1，a5）<ul>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730194853095.png" alt="image-20240730194853095"></li>
</ul>
</li>
</ul>
</li>
<li><p>之后类似于<strong>策略迭代算法</strong>，进行<strong>Policy improvemt</strong></p>
<ul>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240730195041697.png" alt="image-20240730195041697"></li>
</ul>
</li>
</ul>
<h3 id="——第五节课（p4）"><a href="#——第五节课（p4）" class="headerlink" title="——第五节课（p4）"></a>——第五节课（p4）</h3><h4 id="1-MC-Exploring-Starts"><a href="#1-MC-Exploring-Starts" class="headerlink" title="1.MC Exploring Starts"></a>1.MC Exploring Starts</h4><p>​	<code>exploring starts其实不是一种算法的名字,这个是一个条件:为了确保所有action都被探索到, 简单的想法是确保每个(s, a)都发起(Start)一个episode</code></p>
<ul>
<li><p>MC Basic因为太简单，所以它的效率很低（原因是他要不断的计算action value然后取期望）。</p>
</li>
<li><p>MC Basic的缺陷：首先我们先定义一个visit：state-action pair，也叫做状态—行动对。对于MC Basic而言对于一个episode而言：MC Basi使用处理数据的</p>
<p>方法叫做：Initial-visit method，在这个 episode中我们只会去估计q<sub>Π</sub>(S1,S2),对于数据是一种浪费。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802155601658.png" alt="image-20240802155601658"></p>
</li>
<li><p>浪费的原因：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802160026036.png" alt="image-20240802160026036"></p>
<ul>
<li>数据计算高效的方法：<ul>
<li>1.first-visit method：相当于有效样本仅仅是开始访问的（s，a），对于后面出现的（s，a）都不算在内。</li>
<li>2.every-visit method：相当于在当前样本中涉及的所有以任意（s，a）为起始的discounted return都可以当做有效样本利用。</li>
</ul>
</li>
</ul>
</li>
<li><p>如何更加高效的更新策略：</p>
<ul>
<li>原来的MC Basic是是在等<strong>所有episode得到后计算return算出q值</strong>，才进行更新的，这样有个缺点，就是需要等所有的episode都得到才能更新，有一个等待的过程。</li>
<li>MC Exploring Starts就不需要等待，当一个episode得到后就计算q值，开始更新。这样的话效率就会提升。<ul>
<li><strong>有人可能会说，一个episode的返回不能准确地接近相应的动作值q，事实上，我们已经在截断策略迭代算法中这样做了。</strong>并没有什么问题</li>
</ul>
</li>
</ul>
</li>
<li><p>MC exploring starts的需要理由：</p>
<ul>
<li>区别于MC Basic：<strong>然后exploring里面每个(s,a)只需要进行一次实验得到一个episode，不需要多个episode求平均值</strong></li>
<li><strong>应该还有 MC exploring visit，但是不能保证能够经过所有的<u>（S，a）</u>。</strong>因此算法还可以改进在下一节课。</li>
</ul>
</li>
</ul>
<h4 id="——第五节课（p5）"><a href="#——第五节课（p5）" class="headerlink" title="——第五节课（p5）"></a>——第五节课（p5）</h4><h4 id="1-MC-ε-Greedy-算法"><a href="#1-MC-ε-Greedy-算法" class="headerlink" title="1.MC ε-Greedy 算法"></a>1.MC ε-Greedy 算法</h4><ul>
<li>为了将Exploring Strats这个条件去掉：我们需要引入soft-policy（对每个action都有可能做选择） <ul>
<li>这样我们就可以不像 Exploring Starts，将所有(s, a)无论概率如何都遍历一遍才能保证不会遗漏action。我们现在只需要从一个或者几个（s，a）出发就可以覆盖到其他所有的（s，a）。</li>
</ul>
</li>
<li>那在这里我们就是用<strong>soft-policy</strong>里的<strong>ε-Greedy policy</strong>（就避免像greedy action那样某个action的概率是0% ）<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802204112509.png" alt="image-20240802204112509"></li>
<li>选择greedy(最优action)的概率 始终比选择其他action的概率要大或者等于。</li>
<li><strong>ε是一个可调参数</strong></li>
<li>ε-greedy用来平衡exploitation（充分利用）和exploration（探索）<ul>
<li>exploitation是指从当前s出发，以较大的概率选择action中action value最大的。</li>
<li>exploration是指从当前s出发，虽然能以较大的概率选择得到最大的action value的action，但是有可能目前信息是不完全的，有可能其他aciton的action value要更好，应该要有概率去探索一下其他的action。</li>
</ul>
</li>
</ul>
<h4 id="2-如何将ε-greedy和蒙特卡洛结合"><a href="#2-如何将ε-greedy和蒙特卡洛结合" class="headerlink" title="2.如何将ε-greedy和蒙特卡洛结合"></a>2.如何将ε-greedy和蒙特卡洛结合</h4><ul>
<li>之前MC Basic 和MC Exploring Stars的策略改进都是利用的greedy action <img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240802211049997.png" alt="image-20240802211049997"></li>
<li>这里用的是every-visit，而不是first-visit。因为可能会有一个非常长的episode ，如果只用first-visit,许多state-action pair实际上被访问很多次，就会有些浪费。</li>
</ul>
<h3 id="——第五节课（p6）"><a href="#——第五节课（p6）" class="headerlink" title="——第五节课（p6）"></a>——第五节课（p6）</h3><h4 id="1-MC-ε-Greedy-算法的特点"><a href="#1-MC-ε-Greedy-算法的特点" class="headerlink" title="1.MC ε-Greedy 算法的特点"></a>1.MC ε-Greedy 算法的特点</h4><ul>
<li>当越小时，探索更弱，充分利用更强。如果ε &#x3D; 0. 那么会始终选择greedy (最优action)。</li>
<li>当越大时，探索更强，充分利用更弱。如果ε &#x3D; 1. 那么选择所有action的概率都相同 (均匀分布)</li>
<li>MC ε-Greedy 算法的优缺点：<ul>
<li><strong>优点</strong>：具有一定的探索性, 即使只有一个episode , 它也会尝试多数的action</li>
<li><strong>缺点</strong>：牺牲了最优性, 得到的策略<strong>可能</strong>不是最优的, 这点与之前所有的算法都不同</li>
<li>ε的值越小, 生成的策略可能接近最优策略, 所以在实际中可以在前期让ε较大获得探索性, 在后期让ε较小获得最优性。</li>
</ul>
</li>
</ul>
<h4 id="2-MC-basic，MC-exploring-starts与MC-ε-Greedy-的理解"><a href="#2-MC-basic，MC-exploring-starts与MC-ε-Greedy-的理解" class="headerlink" title="2.MC basic，MC exploring starts与MC ε-Greedy 的理解"></a>2.MC basic，MC exploring starts与MC ε-Greedy 的理解</h4><ul>
<li><strong>MC basic</strong>:MC basic是用采样的方式得到的Q值，通常从固定的初始状态开始。每采样一次就能得到一个Q的估计，最后对所有的Q取平均，采样的越多对于当前Q就越接近，mc basic是对于每一个&lt;state,action&gt; pair都会进行一个估计。</li>
<li><strong>MC exploring starts</strong>：引入了一个随机性，MC exploring starts，不是一种新方法，它对于策略引入了一个随机性，这个随机性得保证在你在起始点，采样足够长（一百万步一千万步），能把所有的pairs都采集到，这样对于每一次访问到就相当于对每一个访问进行一个MC采样。<u>至于用这个采样只用最长那个，还是说把所有采样进行一个平均，这就是first visit和every visit的区别。</u></li>
<li><strong>MC ε-Greedy</strong>： 更像MC basic。MC ε-Greedy是一种思想了，它没有一种必须在什么时候用，或者必须用或者必须不用，就是你想增加探索的时候可以用它，它在epsilon 趋于零的时候，它的最优性以及和最优策略的一致性是能得到保证的。</li>
</ul>
<h3 id="——第六节课（p1）"><a href="#——第六节课（p1）" class="headerlink" title="——第六节课（p1）"></a>——第六节课（p1）</h3><h4 id="1-mean-estimaton"><a href="#1-mean-estimaton" class="headerlink" title="1.mean estimaton"></a>1.mean estimaton</h4><p>​	通过大量的数据采样，并且通过计算平均值，来计算期望值。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805153606163.png" alt="image-20240805153606163"></p>
<h4 id="2-如何计算期望"><a href="#2-如何计算期望" class="headerlink" title="2.如何计算期望"></a>2.如何计算期望</h4><ul>
<li><p><strong>方法一：</strong>相加除以N。</p>
<ul>
<li>缺点：对于采样的数据我们需要去等很久。</li>
</ul>
</li>
<li><p><strong>方法二：</strong>通过迭代的方式。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805153927936.png" alt="image-20240805153927936"></p>
</li>
</ul>
<p>​                  	那么W<sub>K+1</sub>的值便是：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805154020741.png" alt="image-20240805154020741"></p>
<p>​		              然后通过迭代得到：<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240805184119814.png" alt="image-20240805184119814"></p>
<h3 id="——第六节课（p2）"><a href="#——第六节课（p2）" class="headerlink" title="——第六节课（p2）"></a>——第六节课（p2）</h3><h4 id="1-Stochastic-approximate（随即近似算法，简称：SA）"><a href="#1-Stochastic-approximate（随即近似算法，简称：SA）" class="headerlink" title="1.Stochastic approximate（随即近似算法，简称：SA）"></a>1.Stochastic approximate（随即近似算法，简称：SA）</h4><p>​	SA表示一大类算法，这些算法是要进行方程的求解或者是优化问题</p>
<h4 id="2-Robbins-Monro-RM-算法"><a href="#2-Robbins-Monro-RM-算法" class="headerlink" title="2.Robbins-Monro (RM) 算法:"></a>2.<strong>Robbins-Monro (RM) 算法:</strong></h4><p>RM算法的精髓：通过不断的输入w，来判断g（w）是否等于0，若g（w）不等于0，则通过式子调整下一个输入的w，精髓就在下一次输入的w与上一次输入的w差值有多少，也就是所减去的<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240821165316711.png" alt="image-20240821165316711">。</p>
<ul>
<li><p>著名的随机梯度下降算法（stochastic gradient descent algorithm）是 RM 算法的一种特殊形式。</p>
</li>
<li><p>它可以用来分析开头介绍的均值估计（mean estimation）算法。我们前面介绍的 mean estimation 算法也是一种特殊的 RM 算法。</p>
</li>
<li><p>在求解”函数 g 的表达式未知“的式子中，我们想得到<strong>g(w)&#x3D;0</strong>的最优解，我们利用的就是RM算法。</p>
<ul>
<li><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240821161808433.png" alt="image-20240821161808433"></p>
</li>
<li><p>RM算法是个迭代式的算法，对 w* 第 k 次的估计是 wk，第 k+1 次的估计是 wk+1。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240821161745086.png" alt="image-20240821161745086"></p>
</li>
</ul>
</li>
</ul>
<p>​		</p>
<h3 id="——第六节课（p3）"><a href="#——第六节课（p3）" class="headerlink" title="——第六节课（p3）"></a>——第六节课（p3）</h3><h4 id="1-RM-算法为什么能够收敛？"><a href="#1-RM-算法为什么能够收敛？" class="headerlink" title="1.RM 算法为什么能够收敛？"></a>1.RM 算法为什么能够收敛？</h4><p>三个条件保证收敛：</p>
<ul>
<li>对g（w）的梯度的要求  <ul>
<li>这条曲线必须是递增的，递减的不行。</li>
<li>梯度也要小于某一个值。</li>
</ul>
</li>
<li>对系数a<sub>k</sub>的要求（a<sub>k</sub>&#x3D;k分之一最佳：满足下面的条件）<ul>
<li>确保k趋近于无穷时，a<sub>k</sub>收敛到0。</li>
<li>确保a<sub>k</sub>不要收敛的太快。</li>
</ul>
</li>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20240912110457828.png" alt="image-20240912110457828"><ul>
<li>η的期望是0，并且η的<strong>方差</strong>是有界的。</li>
<li>并且观测误差η<sub>k</sub>并不是个高斯噪音。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean estimation 算法其实是特殊的RM的算法</span><br></pre></td></tr></table></figure>

<h3 id="——第六节课（p4）"><a href="#——第六节课（p4）" class="headerlink" title="——第六节课（p4）"></a>——第六节课（p4）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SGD是RM算法的特殊情况，mean estimation算法也是SGD的特殊情况</span><br></pre></td></tr></table></figure>

<h4 id="1-SGD算法解决的问题"><a href="#1-SGD算法解决的问题" class="headerlink" title="1.SGD算法解决的问题"></a>1.SGD算法解决的问题</h4><p>假设我们的目标是解决以下优化问题：</p>
<p>​                                                                     	<img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005102825893.png" alt="image-20241005102825893"></p>
<ul>
<li>目标函数 J，是 w 的函数，目标是要找到最优的 w，要优化 w 使得目标函数达到<strong>最小</strong>。</li>
<li>标函数是 f 的期望，f 是 w 和随机变量 X 的函数。随机变量 X 的概率分布（probability distribution）已经给定，但我们还不知道。期望是对 X 求期望。</li>
</ul>
<h4 id="2-解决该问题的方法"><a href="#2-解决该问题的方法" class="headerlink" title="2.解决该问题的方法"></a>2.解决该问题的方法</h4><p>方法1：<strong>梯度下降（gradient descent，GD）</strong></p>
<p>因为我们的目标是最小化一个目标函数，所以要用梯度下降；如果目标是最大化一个目标函数，就要用梯度上升。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005103214111.png" alt="image-20241005103214111"></p>
<p>缺点：难以获得期望值（expected value）。对此有两种解决方法：第一种方法，如果有模型就可以求出来；第二种方法，如果没有模型，用数据求。</p>
<p>方法二：<strong>批量梯度下降（batch gradient descent，BGD）</strong></p>
<p>​	对随机变量f（w<sub>k</sub>,x)进行采样，然后进行求平均值也认为期望。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005103533830.png" alt="image-20241005103533830"></p>
<p>缺点：每次迭代都需要对每个 wk 进行多次采样。在每次更新 wk 的时候都要采样 n 次或者多次。这在实际中还是不实用。</p>
<p>方法三：<strong>随机梯度下降（stochastic gradient descent，SGD）</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005103706760.png" alt="image-20241005103706760"></p>
<p>在 batch gradient descent （ BGD）中，要采样很多次，如果采样越多，对期望估计的就越准，但是问题是也需要很多数据；在 stochastic gradient descent （SGD）中，只用了一个数据</p>
<h3 id="——第六节课（p5）"><a href="#——第六节课（p5）" class="headerlink" title="——第六节课（p5）"></a>——第六节课（p5）</h3><h4 id="1-举例说明SGD"><a href="#1-举例说明SGD" class="headerlink" title="1.举例说明SGD"></a>1.举例说明SGD</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005154944756.png" alt="image-20241005154944756"></p>
<p>问题1：<strong>证明最优解为 w* &#x3D; E[X].</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005155918960.png" alt="image-20241005155918960"></p>
<p>问题2：<strong>写出解决这个问题的 GD 算法。</strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005160014555.png" alt="image-20241005160014555"></p>
<p>问题3：<strong>写出解决这个问题的 SGD 算法。</strong></p>
<p>  先知道 gradient descent，GD 算法，知道里面要有期望，<strong>把期望去掉</strong>就直接得到 SGD</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005160100252.png" alt="image-20241005160100252"></p>
<h4 id="2-为什么-SGD-能够是有效的？"><a href="#2-为什么-SGD-能够是有效的？" class="headerlink" title="2.为什么 SGD 能够是有效的？"></a>2.为什么 SGD 能够是有效的？</h4><p>​	SGD 的基本思路就是从 GD 出发， GD 中的<strong>期望 E</strong> 是不知道的；因此我们干脆把它去掉，用一个<strong>采样</strong>来近似这个 E。这个就是 SGD。</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005161632677.png" alt="image-20241005161632677"></p>
<p>用 stochastic gradient 去近似 true gradient，那么它们之间肯定是存在一个误差的，他俩之间的关系式如下：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005161655914.png" alt="image-20241005161655914"></p>
<p>​		   <em><em><u>stochastic gradient 肯定不是准确的，在这种情况下，用 SGD 是否能找到最优解 w</em> 呢？</u></em>*</p>
<p>解释：基本思想是证明 SGD 是一个特殊的 RM 算法，<strong>RM 算法在满足一定条件下是可以收敛的</strong>，我们就知道 SGD 在满足什么条件下也是能够收敛的。</p>
<p>如何证明SGD是一个特殊的RM算法：</p>
<ul>
<li><p>SGD 要解决的问题是去最小化下面这样一个目标函数（objective function）</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164136364.png" alt="image-20241005164136364"></p>
</li>
<li><p>这个优化问题可以转化为一个寻根问题（a root-finding problem），就是求解一个方程的问题，因为上面的目标函数要达到最优的话，必要条件是<strong>它的梯度（gradient）等于 0：</strong>（区别于RM算法）</p>
</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164318206.png" alt="image-20241005164318206"></p>
<ul>
<li>如果让 g(w) 等于梯度（gradient），那么求解上上个图片的最优问题就变成了求解一个方程 g(w)&#x3D;0 的问题。</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164428362.png" alt="image-20241005164428362"></p>
<p><strong>那么 g(w)&#x3D;0 可以用一个 RM 算法来求解</strong>。</p>
<p>为了求解 RM 算法需要用到数据，也就是 g(w) 算法的表达式我们不知道，但是<strong>我们有一些测量</strong>。g~ 是 stochastic gradient：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164720680.png" alt="image-20241005164720680"></p>
<p>所以，求解 g(w)&#x3D;0 的 RM 算法是：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005164808120.png" alt="image-20241005164808120"></p>
<p>所以这个 RM 算法就是一个 SGD 算法，反过来说，SGD 算法是求解这样一个特殊问题的 RM 算法，相应的收敛性也可以用 RM 算法的收敛性结论来分析。</p>
<h3 id="——第六节课（p6）"><a href="#——第六节课（p6）" class="headerlink" title="——第六节课（p6）"></a>——第六节课（p6）</h3><h4 id="1-随机梯度和批量梯度之间的相对误差（relative-error）"><a href="#1-随机梯度和批量梯度之间的相对误差（relative-error）" class="headerlink" title="1.随机梯度和批量梯度之间的相对误差（relative error）"></a>1.随机梯度和批量梯度之间的相对误差（relative error）</h4><p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005200630237.png" alt="image-20241005200630237"></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005202127699.png" alt="image-20241005202127699"></p>
<p>并且我们在最后的式子中注意到：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005202220565.png" alt="image-20241005202220565"></p>
<p>上式表明，SGD 的收敛模式非常有趣：</p>
<ul>
<li>当 w<sub>k</sub> 距离 w* **<u>比较远的时候</u><strong>，分母比较大，若分子是有限的，</strong><u>那么相对误差（relative error）比较小</u><em><em>，也就是 stochastic gradient 和 true gradient 比较接近，这时候SGD 和 GD 很类似，也就是它会大概朝这个方向接近目标；相反，如果 wk-w</em> 比较小的时候，整个上界比较大，w<sub>k</sub> 在 w</em> 的附近，这时候存在随机性</li>
<li>当 |w<sub>k</sub> - w∗| 较大时——<strong>即两者距离较小时</strong>，δk 较小，SGD 的表现与普通的梯度下降（GD）相似</li>
<li>当 <sub>k</sub> 接近 w∗ 时，相对误差可能很大，收敛在 w∗ 附近表现出更大的随机性。</li>
</ul>
<h4 id="2-举例说明SGD的收敛模式"><a href="#2-举例说明SGD的收敛模式" class="headerlink" title="2.举例说明SGD的收敛模式"></a>2.举例说明SGD的收敛模式</h4><ul>
<li>设置 X∈R*2 表示平面上的一个随机位置。其在以原点为中心、边长为 20 的正方形区域内分布均匀。真实均值为 E[X] &#x3D;[0,0]T（转置）。均值估计基于 100 个独立同分布的（iid）样本 {xi}，i的取值范围1-100。</li>
<li>在以原点为中心、边长为 20 的正方形区域内均匀随机采样 100 组，采集 100 组点。用这 100 组点跑刚才的 mean estimation 的算法，也就是 w<sub>k+1</sub>&#x3D;w<sub>k</sub>-α<sub>k</sub>(w<sub>k</sub>-x<sub>k</sub>)，观察整个算法在收敛过程中呈现什么样的行为</li>
</ul>
<p>结果如下：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241005210245699.png" alt="image-20241005210245699"></p>
<ul>
<li>虽然初始猜测的平均值与真实值相距甚远，但 SGD 估计值<strong>可以快速接近真实值</strong>的邻域。</li>
<li>当估计值接近真值时，<strong>它表现出一定的随机性</strong>，但仍会逐渐接近真值。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">离得越远反而不会表现出随机性，越靠近真值时，才会表现出一定的随机性。</span><br></pre></td></tr></table></figure>

<h4 id="3-SGD的一种确定性表述"><a href="#3-SGD的一种确定性表述" class="headerlink" title="3.SGD的一种确定性表述"></a>3.SGD的一种确定性表述</h4><p><strong>情景：</strong></p>
<ul>
<li>我们上面介绍的 SGD 公式（The formulation of SGD）涉及<strong>随机变量</strong>和<strong>期望值</strong>（random variables and expectation）。但是我们经常会遇到不涉及任何随机变量（random variables）的确定性 SGD 公式。</li>
</ul>
<p>我们需要考虑的优化问题：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007162537278.png" alt="image-20241007162537278"></p>
<p>我们求解这个问题的梯度下降的方法是：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007162739098.png" alt="image-20241007162739098"></p>
<p>但是假设集合 {xi} 很大，我们每次只能获取一个数字 xi。在这种情况下，我们可以使用下面的迭代算法：把上式求平均的 xi 用 xk 代替。</p>
<p>这里就会产问题:</p>
<p><strong>问题：</strong></p>
<ul>
<li>这个算法看起来与 SGD 非常类似，但这种算法是 SGD 吗？</li>
<li>并且在这个集合里面每拿一个数出来作为X<sub>k</sub>,我们该怎么拿？</li>
</ul>
<p><strong>解决方法：</strong></p>
<ul>
<li>我们可以手动引入一个随机变量（introduce a random variable manually），将刚才一个不涉及随机变量的问题变成一个涉及随机变量的问题，将 SGD 的确定性公式（deterministic formulation）转换为随机公式（stochastic formulation）。</li>
<li>所以，从集合 {xi} 中抽取 xk，<strong>应该随机抽取而不应该排序</strong>，而且因为是随机抽取，所以<strong>集合里面的这些数字会被反复用到。</strong></li>
<li><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007163244515.png" alt="image-20241007163244515"></li>
</ul>
<h3 id="——第六节课（p7）"><a href="#——第六节课（p7）" class="headerlink" title="——第六节课（p7）"></a>——第六节课（p7）</h3><h4 id="1-辨析BGD、MBGD、SGD"><a href="#1-辨析BGD、MBGD、SGD" class="headerlink" title="1.辨析BGD、MBGD、SGD"></a>1.辨析BGD、MBGD、SGD</h4><p>情景：有一个目标函数 J(w)，有 n 个采样 X<sub>i</sub>，我要用这样一组数据优化目标函数，有三种方法：BGD，MBGD 和 SGD</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007193734159.png" alt="image-20241007193734159"></p>
<ul>
<li><strong>BGD</strong> 每次都要用到 n 个所有的采样，在这个基础上求平均，这个可以说最接近于真实的期望（expectation）</li>
<li><strong>MBGD</strong> 不用所有的采样，每次只用全部采样中的一部分，选取一组数使用（总集合的子集）</li>
<li><strong>SGD</strong> 就是从集合 {X<sub>i</sub>} 中随机选择一个 采样出来。</li>
</ul>
<p><strong>比较 MBGD，BGD和 SGD（MBGD其实包括了BGD与SGD）</strong></p>
<ul>
<li>与 SGD 相比，MBGD 的随机性更小，因为它使用了更多的样本，而不是像 SGD 那样只使用一个样本。用更多的数据去平均的话，会把噪音等等测量给平均掉。</li>
<li>与 BGD 相比，MBGD 的数据较少，它的随机性会更大一些，因为它无需在每次迭代中使用所有样本，因此更加灵活高效。</li>
<li>如果 MBGD 的 mini-batch m &#x3D; 1，则 MBGD 变为 SGD。如果 MBGD 的 mini-batch m &#x3D; n，严格来说，MBGD 不会变成 BGD，<strong>因为 MBGD 使用随机获取的 n 个样本</strong>，抽取的过程中可能一个采样抽取了好几次没有抽到，而 BGD 使用所有 n 个数字。</li>
</ul>
<h4 id="2-举例说明-1"><a href="#2-举例说明-1" class="headerlink" title="2.举例说明"></a>2.举例说明</h4><p>有 n 个数字，我们的目标是求平均值，求平均值的问题可以等价为一个优化问题：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007204638567.png" alt="image-20241007204638567"></p>
<h4 id="3-第六节课总结"><a href="#3-第六节课总结" class="headerlink" title="3.第六节课总结"></a>3.第六节课总结</h4><ul>
<li>介绍了 mean estimation 算法，用一组数来求期望（expectation），之前我们只是简单的把这一组数求平均，就可以近似估计它的期望；这节课讲了一个迭代的算法，当我得到一个采样，就计算一次，这样就不用等所有的采样全部拿到了再计算，会更加高效。</li>
<li>介绍了非常经典的 SA 领域的 RM 算法，要求的是方程 g(w)&#x3D;0 的最优解 w<em>，但是不知道 g(w) 的表达式，只知道给一个 w 能测出来输出，而且这个输出的有噪音或者有误差的，输出用 g~ 表示，所以 RM 算法是怎么样用含有噪音的测量来估计 w</em></li>
<li>介绍了 SGD 算法，知道目标函数 J(w)，还知道它的一个梯度的采样，就可以用 stochastic gradient 这个算法让最后的 Wk 趋于最优值W*</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241007205121874.png" alt="image-20241007205121874"></p>
<h3 id="——第七节课（p1）"><a href="#——第七节课（p1）" class="headerlink" title="——第七节课（p1）"></a>——第七节课（p1）</h3><h4 id="1-例子引出"><a href="#1-例子引出" class="headerlink" title="1.例子引出"></a>1.例子引出</h4><p>​	第六次课，我们考虑简单的均值估计问题（mean estimation problem）：（之前的课程都在反复讲这个问题，我们会从不同角度介绍这个问题）：mean estimation 问就是要求解一个 random variable X 的 expectation，用 w 来表示这样一个 expectation 的值，现在有的是 X 的独立同分布采样，用这些采样求 expectation。这个问题有很多算法可以求解，这里讲解 RM 算法<br><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021201410489.png" alt="image-20241021201410489"></p>
<p>后面我们将看到 TD 算法也有类似的表达式。与RM算法类似</p>
<h3 id="——第七节课（p2）"><a href="#——第七节课（p2）" class="headerlink" title="——第七节课（p2）"></a>——第七节课（p2）</h3><h4 id="1-TD-算法（估计state-value）"><a href="#1-TD-算法（估计state-value）" class="headerlink" title="1.TD 算法（估计state value）"></a>1.TD 算法（估计state value）</h4><ul>
<li><p><strong>TD 算法是基于数据，不基于模型实现强化学习的，算法使用的数据&#x2F;经验（data&#x2F;experience）如下：*<em>这些数据全都是由一个给定的策略 π 所产生的，下面的 TD 算法就是要用这些数据估计 π 所对应的 state value*</em></strong></p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021205232188.png" alt="image-20241021205232188"></p>
</li>
<li><p>下面 TD 算法就是要用这些数据来估计 π 对应的 state value，<strong>下面正式的给出 TD 算法，包含两个式子：</strong></p>
</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021205404138.png" alt="image-20241021205404138"></p>
<p>对于上述的式子有一点非常重要！！！</p>
<ul>
<li><strong>在时间t，只有访问状态st的值被更新，而未访问状态S≠St的值保持不变。</strong></li>
</ul>
<p>下面详细介绍第一个式子：</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021210142657.png" alt="image-20241021210142657"></p>
<p>下面详细介绍上面式子中的两项：</p>
<ul>
<li>如何理解TD target</li>
</ul>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021211458012.png" alt="image-20241021211458012"></p>
<ul>
<li><p>如何理解TD error</p>
<p><img src="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20241021213016099.png" alt="image-20241021213016099"></p>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/07/25/pytorch%E5%AE%89%E8%A3%85jupyter%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/" rel="prev" title="在pytorch环境中安装jupyter所遇到的问题">
      <i class="fa fa-chevron-left"></i> 在pytorch环境中安装jupyter所遇到的问题
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.1.</span> <span class="nav-text">——第一节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-agent"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.agent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-state"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.state</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-action"><span class="nav-number">1.1.3.</span> <span class="nav-text">3.action</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-state-transition"><span class="nav-number">1.1.4.</span> <span class="nav-text">4.state transition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-forbidden-area"><span class="nav-number">1.1.5.</span> <span class="nav-text">5.forbidden area</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-state-transition-probability"><span class="nav-number">1.1.6.</span> <span class="nav-text">6.state transition probability</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-policy"><span class="nav-number">1.1.7.</span> <span class="nav-text">7.policy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">——第一节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-reward"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.reward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-trajectory"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.trajectory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-return"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.return</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-discounted-return"><span class="nav-number">1.2.4.</span> <span class="nav-text">4.discounted return</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-episode"><span class="nav-number">1.2.5.</span> <span class="nav-text">5.episode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.3.</span> <span class="nav-text">——第二节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%B8%BE%E4%BE%8B"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.举例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97return"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.如何计算return</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E7%9A%84%E6%8E%A8%E5%AF%BC%EF%BC%88%E7%89%B9%E6%AE%8A%E6%80%A7%EF%BC%89"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.贝尔曼公式的推导（特殊性）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.4.</span> <span class="nav-text">——第二节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-state-value"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.state value</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BE%E4%BE%8B"><span class="nav-number">1.4.2.</span> <span class="nav-text">2.举例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.5.</span> <span class="nav-text">——第二节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E6%99%AE%E9%81%8D%E6%80%A7%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.5.1.</span> <span class="nav-text">1.贝尔曼公式普遍性推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p4%EF%BC%89"><span class="nav-number">1.6.</span> <span class="nav-text">——第二节课（p4）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E5%BD%A2%E5%BC%8F%EF%BC%88matrix-vector-form%EF%BC%89%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.6.1.</span> <span class="nav-text">1.贝尔曼矩阵向量形式（matrix-vector form）的推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%8C%E8%8A%82%E8%AF%BE%EF%BC%88p5%EF%BC%89"><span class="nav-number">1.7.</span> <span class="nav-text">——第二节课（p5）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-state-value%E5%92%8Caction-value%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.7.1.</span> <span class="nav-text">1.state value和action value的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-action-value%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">1.7.2.</span> <span class="nav-text">2.action value的定义</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.8.</span> <span class="nav-text">——第三节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E5%BC%95%E5%87%BA%E2%80%9D%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%E2%80%9C"><span class="nav-number">1.8.1.</span> <span class="nav-text">1.通过例子引出”贝尔曼最优公式“</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.9.</span> <span class="nav-text">——第三节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="nav-number">1.9.1.</span> <span class="nav-text">1.贝尔曼最优公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="nav-number">1.9.2.</span> <span class="nav-text">2.如何求解贝尔曼最优公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.10.</span> <span class="nav-text">——第三节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%B0%86%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%E5%86%99%E6%88%90v-f-v"><span class="nav-number">1.10.1.</span> <span class="nav-text">1.将贝尔曼最优公式写成v&#x3D;f(v)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Contraction-mapping-theorem%EF%BC%88%E6%94%B6%E7%BC%A9%E6%98%A0%E5%B0%84%E5%AE%9A%E7%90%86%EF%BC%89"><span class="nav-number">1.10.2.</span> <span class="nav-text">2.Contraction mapping theorem（收缩映射定理）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%89%E8%8A%82%E8%AF%BE%EF%BC%88p4%EF%BC%89"><span class="nav-number">1.11.</span> <span class="nav-text">——第三节课（p4）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%BD%B1%E5%93%8D%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0"><span class="nav-number">1.11.1.</span> <span class="nav-text">1.影响贝尔曼最优公式的影响因素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%80%9A%E8%BF%87%E4%BF%AE%E6%94%B9%E2%80%9Creward%E2%80%9D%E5%92%8C%E2%80%9C%CE%B3%E2%80%9D%E4%BC%9A%E6%94%B9%E5%8F%98%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="nav-number">1.11.2.</span> <span class="nav-text">2.通过修改“reward”和“γ”会改变最优策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5%E7%9A%84%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="nav-number">1.11.3.</span> <span class="nav-text">3.最优策略的不变性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%9B%9B%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.12.</span> <span class="nav-text">——第四节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95%EF%BC%88Value-interation%EF%BC%89"><span class="nav-number">1.12.1.</span> <span class="nav-text">1.值迭代算法（Value interation）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.12.2.</span> <span class="nav-text">2.值迭代的步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="nav-number">1.12.3.</span> <span class="nav-text">3.举例说明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%9B%9B%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.13.</span> <span class="nav-text">——第四节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95%EF%BC%88policy-evaluation%EF%BC%89"><span class="nav-number">1.13.1.</span> <span class="nav-text">1.策略迭代算法（policy evaluation）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="nav-number">1.13.2.</span> <span class="nav-text">2.举例说明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%9B%9B%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.14.</span> <span class="nav-text">——第四节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.14.1.</span> <span class="nav-text">1.策略迭代算法与价值迭代算法的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%88%AA%E6%96%AD%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%EF%BC%88Iteration-of-truncation-strategies%EF%BC%89"><span class="nav-number">1.14.2.</span> <span class="nav-text">2.截断策略迭代（Iteration of truncation strategies）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%94%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.15.</span> <span class="nav-text">——第五节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B"><span class="nav-number">1.15.1.</span> <span class="nav-text">1.通过例子介绍蒙特卡洛</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%94%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.16.</span> <span class="nav-text">——第五节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-MC-basic-%E7%AE%97%E6%B3%95%EF%BC%88%E8%B5%B5%E8%80%81%E5%B8%88%E8%87%AA%E5%B7%B1%E5%86%99%E5%87%BA%E7%9A%84%E7%AE%97%E6%B3%95%EF%BC%8C%E4%BE%BF%E4%BA%8E%E7%90%86%E8%A7%A3%E7%9A%84%EF%BC%89"><span class="nav-number">1.16.1.</span> <span class="nav-text">1.MC basic 算法（赵老师自己写出的算法，便于理解的）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%B1%82%E8%A7%A3MC-basic%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.16.2.</span> <span class="nav-text">2.求解MC basic的步骤</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%94%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.17.</span> <span class="nav-text">——第五节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-MC-Basic%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">1.17.1.</span> <span class="nav-text">1.MC Basic的例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%94%E8%8A%82%E8%AF%BE%EF%BC%88p4%EF%BC%89"><span class="nav-number">1.18.</span> <span class="nav-text">——第五节课（p4）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-MC-Exploring-Starts"><span class="nav-number">1.18.1.</span> <span class="nav-text">1.MC Exploring Starts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%94%E8%8A%82%E8%AF%BE%EF%BC%88p5%EF%BC%89"><span class="nav-number">1.18.2.</span> <span class="nav-text">——第五节课（p5）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-MC-%CE%B5-Greedy-%E7%AE%97%E6%B3%95"><span class="nav-number">1.18.3.</span> <span class="nav-text">1.MC ε-Greedy 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E5%B0%86%CE%B5-greedy%E5%92%8C%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%BB%93%E5%90%88"><span class="nav-number">1.18.4.</span> <span class="nav-text">2.如何将ε-greedy和蒙特卡洛结合</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%BA%94%E8%8A%82%E8%AF%BE%EF%BC%88p6%EF%BC%89"><span class="nav-number">1.19.</span> <span class="nav-text">——第五节课（p6）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-MC-%CE%B5-Greedy-%E7%AE%97%E6%B3%95%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">1.19.1.</span> <span class="nav-text">1.MC ε-Greedy 算法的特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-MC-basic%EF%BC%8CMC-exploring-starts%E4%B8%8EMC-%CE%B5-Greedy-%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-number">1.19.2.</span> <span class="nav-text">2.MC basic，MC exploring starts与MC ε-Greedy 的理解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.20.</span> <span class="nav-text">——第六节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-mean-estimaton"><span class="nav-number">1.20.1.</span> <span class="nav-text">1.mean estimaton</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E6%9C%9F%E6%9C%9B"><span class="nav-number">1.20.2.</span> <span class="nav-text">2.如何计算期望</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.21.</span> <span class="nav-text">——第六节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Stochastic-approximate%EF%BC%88%E9%9A%8F%E5%8D%B3%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95%EF%BC%8C%E7%AE%80%E7%A7%B0%EF%BC%9ASA%EF%BC%89"><span class="nav-number">1.21.1.</span> <span class="nav-text">1.Stochastic approximate（随即近似算法，简称：SA）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Robbins-Monro-RM-%E7%AE%97%E6%B3%95"><span class="nav-number">1.21.2.</span> <span class="nav-text">2.Robbins-Monro (RM) 算法:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p3%EF%BC%89"><span class="nav-number">1.22.</span> <span class="nav-text">——第六节课（p3）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-RM-%E7%AE%97%E6%B3%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%A4%9F%E6%94%B6%E6%95%9B%EF%BC%9F"><span class="nav-number">1.22.1.</span> <span class="nav-text">1.RM 算法为什么能够收敛？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p4%EF%BC%89"><span class="nav-number">1.23.</span> <span class="nav-text">——第六节课（p4）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-SGD%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.23.1.</span> <span class="nav-text">1.SGD算法解决的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%A7%A3%E5%86%B3%E8%AF%A5%E9%97%AE%E9%A2%98%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.23.2.</span> <span class="nav-text">2.解决该问题的方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p5%EF%BC%89"><span class="nav-number">1.24.</span> <span class="nav-text">——第六节课（p5）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8ESGD"><span class="nav-number">1.24.1.</span> <span class="nav-text">1.举例说明SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88-SGD-%E8%83%BD%E5%A4%9F%E6%98%AF%E6%9C%89%E6%95%88%E7%9A%84%EF%BC%9F"><span class="nav-number">1.24.2.</span> <span class="nav-text">2.为什么 SGD 能够是有效的？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p6%EF%BC%89"><span class="nav-number">1.25.</span> <span class="nav-text">——第六节课（p6）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E5%92%8C%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E5%AF%B9%E8%AF%AF%E5%B7%AE%EF%BC%88relative-error%EF%BC%89"><span class="nav-number">1.25.1.</span> <span class="nav-text">1.随机梯度和批量梯度之间的相对误差（relative error）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8ESGD%E7%9A%84%E6%94%B6%E6%95%9B%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.25.2.</span> <span class="nav-text">2.举例说明SGD的收敛模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-SGD%E7%9A%84%E4%B8%80%E7%A7%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E8%A1%A8%E8%BF%B0"><span class="nav-number">1.25.3.</span> <span class="nav-text">3.SGD的一种确定性表述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%EF%BC%88p7%EF%BC%89"><span class="nav-number">1.26.</span> <span class="nav-text">——第六节课（p7）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%BE%A8%E6%9E%90BGD%E3%80%81MBGD%E3%80%81SGD"><span class="nav-number">1.26.1.</span> <span class="nav-text">1.辨析BGD、MBGD、SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E-1"><span class="nav-number">1.26.2.</span> <span class="nav-text">2.举例说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E7%AC%AC%E5%85%AD%E8%8A%82%E8%AF%BE%E6%80%BB%E7%BB%93"><span class="nav-number">1.26.3.</span> <span class="nav-text">3.第六节课总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%83%E8%8A%82%E8%AF%BE%EF%BC%88p1%EF%BC%89"><span class="nav-number">1.27.</span> <span class="nav-text">——第七节课（p1）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BE%8B%E5%AD%90%E5%BC%95%E5%87%BA"><span class="nav-number">1.27.1.</span> <span class="nav-text">1.例子引出</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%83%E8%8A%82%E8%AF%BE%EF%BC%88p2%EF%BC%89"><span class="nav-number">1.28.</span> <span class="nav-text">——第七节课（p2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-TD-%E7%AE%97%E6%B3%95%EF%BC%88%E4%BC%B0%E8%AE%A1state-value%EF%BC%89"><span class="nav-number">1.28.1.</span> <span class="nav-text">1.TD 算法（估计state value）</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ShaoXin Cui"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">ShaoXin Cui</p>
  <div class="site-description" itemprop="description">有志者事竟成</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ShaoXin Cui</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
